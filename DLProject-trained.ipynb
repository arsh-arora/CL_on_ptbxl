{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import time\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data from dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/bmi-lab/Downloads/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3'\n",
    "\n",
    "ptbxl_df = pd.read_csv(os.path.join(DATA_PATH, 'ptbxl_database.csv'))\n",
    "scp_statements = pd.read_csv(os.path.join(DATA_PATH, 'scp_statements.csv'), index_col=0)\n",
    "\n",
    "diagnostic_scps = scp_statements[scp_statements['diagnostic'] == 1].index.values\n",
    "\n",
    "scp_to_superclass = scp_statements['diagnostic_class'].to_dict()\n",
    "scp_to_subclass = scp_statements['diagnostic_subclass'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbxl_df['scp_codes'] = ptbxl_df['scp_codes'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_diagnostic_labels(df, scp_codes, scp_to_agg):\n",
    "    df = df.copy()\n",
    "    def aggregate_labels(scp_codes_dict):\n",
    "        labels = set()\n",
    "        for code in scp_codes_dict.keys():\n",
    "            if code in scp_codes:\n",
    "                label = scp_to_agg.get(code)\n",
    "                if label:\n",
    "                    labels.add(label)\n",
    "        return list(labels)\n",
    "    df['diagnostic_labels'] = df['scp_codes'].apply(aggregate_labels)\n",
    "    return df\n",
    "\n",
    "ptbxl_df = aggregate_diagnostic_labels(ptbxl_df, diagnostic_scps, scp_to_superclass)\n",
    "ptbxl_df = ptbxl_df.rename(columns={'diagnostic_labels': 'superclass_labels'})\n",
    "\n",
    "ptbxl_df = aggregate_diagnostic_labels(ptbxl_df, diagnostic_scps, scp_to_subclass)\n",
    "ptbxl_df = ptbxl_df.rename(columns={'diagnostic_labels': 'subclass_labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbxl_df = ptbxl_df[ptbxl_df['superclass_labels'].map(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = ptbxl_df[ptbxl_df.strat_fold <= 8]\n",
    "val_df = ptbxl_df[ptbxl_df.strat_fold == 9]\n",
    "test_df = ptbxl_df[ptbxl_df.strat_fold == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df, sampling_rate, data_path):\n",
    "    data = []\n",
    "    if sampling_rate == 100:\n",
    "        filenames = df['filename_lr'].values\n",
    "    else:\n",
    "        filenames = df['filename_hr'].values\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(data_path, filename)\n",
    "        signals, _ = wfdb.rdsamp(file_path)\n",
    "        data.append(signals)\n",
    "    return np.array(data)\n",
    "\n",
    "X_train = load_data(train_df, sampling_rate=100, data_path=DATA_PATH)\n",
    "X_val = load_data(val_df, sampling_rate=100, data_path=DATA_PATH)\n",
    "X_test = load_data(test_df, sampling_rate=100, data_path=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_super = train_df['superclass_labels'].values\n",
    "val_labels_super = val_df['superclass_labels'].values\n",
    "test_labels_super = test_df['superclass_labels'].values\n",
    "\n",
    "mlb_super = MultiLabelBinarizer()\n",
    "y_train_super = mlb_super.fit_transform(train_labels_super)\n",
    "y_val_super = mlb_super.transform(val_labels_super)\n",
    "y_test_super = mlb_super.transform(test_labels_super)\n",
    "classes_super = mlb_super.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_sub = train_df['subclass_labels'].values\n",
    "val_labels_sub = val_df['subclass_labels'].values\n",
    "test_labels_sub = test_df['subclass_labels'].values\n",
    "\n",
    "mlb_sub = MultiLabelBinarizer()\n",
    "y_train_sub = mlb_sub.fit_transform(train_labels_sub)\n",
    "y_val_sub = mlb_sub.transform(val_labels_sub)\n",
    "y_test_sub = mlb_sub.transform(test_labels_sub)\n",
    "classes_sub = mlb_sub.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data_per_channel(X):\n",
    "    X = np.transpose(X, (0, 2, 1))\n",
    "    mean = np.mean(X, axis=(0, 2), keepdims=True)\n",
    "    std = np.std(X, axis=(0, 2), keepdims=True)\n",
    "    X = (X - mean) / std\n",
    "    X = np.transpose(X, (0, 2, 1))\n",
    "    return X\n",
    "\n",
    "X_train = normalize_data_per_channel(X_train)\n",
    "X_val = normalize_data_per_channel(X_val)\n",
    "X_test = normalize_data_per_channel(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts_super = np.sum(y_train_super, axis=0)\n",
    "total_samples_super = y_train_super.shape[0]\n",
    "\n",
    "class_weight_super = {}\n",
    "for i, count in enumerate(class_counts_super):\n",
    "    class_weight_super[i] = total_samples_super / (len(class_counts_super) * count)\n",
    "\n",
    "class_counts_sub = np.sum(y_train_sub, axis=0)\n",
    "total_samples_sub = y_train_sub.shape[0]\n",
    "\n",
    "class_weight_sub = {}\n",
    "for i, count in enumerate(class_counts_sub):\n",
    "    class_weight_sub[i] = total_samples_sub / (len(class_counts_sub) * count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_super = y_train_super.shape[1]\n",
    "class_totals = np.sum(y_train_super, axis=0)\n",
    "class_weights = class_totals.max() / class_totals\n",
    "weights_array = np.array(class_weights, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_sub = y_train_sub.shape[1]\n",
    "class_totals_sub = np.sum(y_train_sub, axis=0)\n",
    "class_weights_sub = class_totals_sub.max() / class_totals_sub\n",
    "weights_array_sub = np.array(class_weights_sub, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_super = y_train_super.astype(np.float32)\n",
    "y_val_super = y_val_super.astype(np.float32)\n",
    "y_test_super = y_test_super.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Entropy and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def weighted_binary_crossentropy(weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        weights_cast = K.cast(weights, y_pred.dtype)\n",
    "        y_true = K.cast(y_true, y_pred.dtype)\n",
    "        \n",
    "        bce = K.binary_crossentropy(y_true, y_pred)\n",
    "        weight_vector = y_true * weights_cast + (1 - y_true)\n",
    "        weighted_bce = weight_vector * bce\n",
    "        return K.mean(weighted_bce)\n",
    "    return loss\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    y_pred = K.round(y_pred)\n",
    "    \n",
    "    tp = K.sum(y_true * y_pred, axis=0)\n",
    "    fp = K.sum((1 - y_true) * y_pred, axis=0)\n",
    "    fn = K.sum(y_true * (1 - y_pred), axis=0)\n",
    "\n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2 * precision * recall / (precision + recall + K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv1D(64, kernel_size=7, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(128, kernel_size=5, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(256, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(512, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_resnet_model(input_shape, num_classes):\n",
    "#     inputs = layers.Input(shape=input_shape)\n",
    "#     x = layers.Conv1D(64, kernel_size=7, strides=2, padding='same')(inputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Activation('relu')(x)\n",
    "#     x = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(x)\n",
    "    \n",
    "#     previous_filters = x.shape[-1]\n",
    "#     for filters in [64, 128, 256]:\n",
    "#         x_shortcut = x\n",
    "#         strides = 1\n",
    "#         if previous_filters != filters:\n",
    "#             strides = 2\n",
    "\n",
    "#         x = layers.Conv1D(filters, kernel_size=3, strides=strides, padding='same')(x)\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "#         x = layers.Activation('relu')(x)\n",
    "#         x = layers.Conv1D(filters, kernel_size=3, padding='same')(x)\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        \n",
    "#         if previous_filters != filters or strides != 1:\n",
    "#             x_shortcut = layers.Conv1D(filters, kernel_size=1, strides=strides, padding='same')(x_shortcut)\n",
    "#             x_shortcut = layers.BatchNormalization()(x_shortcut)\n",
    "        \n",
    "#         x = layers.Add()([x, x_shortcut])\n",
    "#         x = layers.Activation('relu')(x)\n",
    "#         previous_filters = filters\n",
    "#     x = layers.GlobalAveragePooling1D()(x)\n",
    "#     outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "#     model = models.Model(inputs, outputs)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block_1d(x, filters, kernel_size=3, strides=1, downsample=False):\n",
    "    shortcut = x\n",
    "    \n",
    "    x = layers.Conv1D(filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv1D(filters, kernel_size=kernel_size, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    if downsample or shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv1D(filters, kernel_size=1, strides=strides, padding='same')(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def create_resnet_model(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(64, kernel_size=7, strides=2, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(x)\n",
    "    layers_filters = [64, 128, 256, 512]\n",
    "    layers_blocks = [3, 4, 6, 3]\n",
    "\n",
    "    for filters, num_blocks in zip(layers_filters, layers_blocks):\n",
    "        for i in range(num_blocks):\n",
    "            if i == 0 and filters != x.shape[-1]:\n",
    "                x = residual_block_1d(x, filters, strides=2, downsample=True)\n",
    "            else:\n",
    "                x = residual_block_1d(x, filters)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def create_vit_model(input_shape, num_classes):\n",
    "    patch_size = 10 \n",
    "    num_patches = input_shape[0] // patch_size\n",
    "    projection_dim = 64\n",
    "    num_heads = 4\n",
    "    transformer_layers = 8\n",
    "    mlp_head_units = [256, 128]\n",
    "    dropout_rate = 0.1\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Reshape((num_patches, patch_size * input_shape[1]))(inputs)\n",
    "    x = layers.Dense(units=projection_dim)(x)\n",
    "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "    position_embedding = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)\n",
    "    x = x + position_embedding(positions)\n",
    "    for _ in range(transformer_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=dropout_rate\n",
    "        )(x1, x1)\n",
    "        x2 = layers.Add()([attention_output, x])\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(x3, hidden_units=[projection_dim * 2, projection_dim], dropout_rate=dropout_rate)\n",
    "        x = layers.Add()([x3, x2])\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, class_weight, batch_size=64, epochs=25):\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', macro_f1]\n",
    "    )\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    ]\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating Models without CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 16:09:31.270097: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-19 16:09:31.549533: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-19 16:09:31.552110: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-19 16:09:31.555656: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-19 16:09:31.559368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-19 16:09:31.561785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-19 16:09:31.563359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-19 16:09:31.664691: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-19 16:09:31.665914: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-19 16:09:31.667141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-19 16:09:31.668300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13813 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 16:09:33.322857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8907\n",
      "2024-11-19 16:09:34.093169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-11-19 16:09:34.191615: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x73d9f8042e20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-11-19 16:09:34.191677: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 4080, Compute Capability 8.9\n",
      "2024-11-19 16:09:34.242857: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-19 16:09:34.547318: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267/267 [==============================] - 5s 7ms/step - loss: 0.2797 - accuracy: 0.6384 - macro_f1: 0.6584 - val_loss: 0.3618 - val_accuracy: 0.6342 - val_macro_f1: 0.6274 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.2422 - accuracy: 0.6853 - macro_f1: 0.7098 - val_loss: 0.3546 - val_accuracy: 0.6696 - val_macro_f1: 0.6617 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.2281 - accuracy: 0.7005 - macro_f1: 0.7294 - val_loss: 0.3061 - val_accuracy: 0.6873 - val_macro_f1: 0.7062 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.2208 - accuracy: 0.7081 - macro_f1: 0.7385 - val_loss: 0.3125 - val_accuracy: 0.6752 - val_macro_f1: 0.6934 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.2149 - accuracy: 0.7142 - macro_f1: 0.7469 - val_loss: 0.3087 - val_accuracy: 0.6887 - val_macro_f1: 0.7001 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.2069 - accuracy: 0.7251 - macro_f1: 0.7559 - val_loss: 0.2876 - val_accuracy: 0.6952 - val_macro_f1: 0.7046 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.2026 - accuracy: 0.7251 - macro_f1: 0.7600 - val_loss: 0.2969 - val_accuracy: 0.6883 - val_macro_f1: 0.7184 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.1972 - accuracy: 0.7325 - macro_f1: 0.7684 - val_loss: 0.2779 - val_accuracy: 0.7213 - val_macro_f1: 0.7196 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1922 - accuracy: 0.7367 - macro_f1: 0.7727 - val_loss: 0.2882 - val_accuracy: 0.7018 - val_macro_f1: 0.7139 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1873 - accuracy: 0.7394 - macro_f1: 0.7805 - val_loss: 0.2828 - val_accuracy: 0.6929 - val_macro_f1: 0.7217 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1846 - accuracy: 0.7413 - macro_f1: 0.7826 - val_loss: 0.2932 - val_accuracy: 0.7008 - val_macro_f1: 0.7212 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1789 - accuracy: 0.7485 - macro_f1: 0.7915 - val_loss: 0.2899 - val_accuracy: 0.6985 - val_macro_f1: 0.7207 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1721 - accuracy: 0.7515 - macro_f1: 0.7963 - val_loss: 0.3178 - val_accuracy: 0.6873 - val_macro_f1: 0.7195 - lr: 0.0010\n",
      "Epoch 14/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1461 - accuracy: 0.7779 - macro_f1: 0.8292 - val_loss: 0.2813 - val_accuracy: 0.7046 - val_macro_f1: 0.7278 - lr: 1.0000e-04\n",
      "Epoch 15/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1363 - accuracy: 0.7858 - macro_f1: 0.8422 - val_loss: 0.2836 - val_accuracy: 0.7088 - val_macro_f1: 0.7249 - lr: 1.0000e-04\n",
      "Epoch 16/25\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.1308 - accuracy: 0.7876 - macro_f1: 0.8482 - val_loss: 0.2915 - val_accuracy: 0.7106 - val_macro_f1: 0.7288 - lr: 1.0000e-04\n",
      "Epoch 17/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1264 - accuracy: 0.7909 - macro_f1: 0.8531 - val_loss: 0.2960 - val_accuracy: 0.7097 - val_macro_f1: 0.7169 - lr: 1.0000e-04\n",
      "Epoch 18/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.1236 - accuracy: 0.7944 - macro_f1: 0.8565 - val_loss: 0.2991 - val_accuracy: 0.7130 - val_macro_f1: 0.7257 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x73dc79970be0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "num_classes_super = y_train_super.shape[1]\n",
    "\n",
    "cnn_super_model = create_cnn_model(input_shape, num_classes_super)\n",
    "train_model(cnn_super_model, X_train, y_train_super, X_val, y_val_super, class_weight_super)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "267/267 [==============================] - 14s 22ms/step - loss: 0.3015 - accuracy: 0.6271 - macro_f1: 0.6294 - val_loss: 0.6704 - val_accuracy: 0.3882 - val_macro_f1: 0.4309 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.2545 - accuracy: 0.6802 - macro_f1: 0.6948 - val_loss: 0.3448 - val_accuracy: 0.6561 - val_macro_f1: 0.6679 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.2375 - accuracy: 0.6924 - macro_f1: 0.7167 - val_loss: 0.4898 - val_accuracy: 0.5415 - val_macro_f1: 0.6120 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.2312 - accuracy: 0.6977 - macro_f1: 0.7230 - val_loss: 0.3757 - val_accuracy: 0.6789 - val_macro_f1: 0.6919 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.2215 - accuracy: 0.7141 - macro_f1: 0.7379 - val_loss: 0.3218 - val_accuracy: 0.6664 - val_macro_f1: 0.6708 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.2150 - accuracy: 0.7179 - macro_f1: 0.7463 - val_loss: 0.4310 - val_accuracy: 0.6300 - val_macro_f1: 0.6252 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.2079 - accuracy: 0.7232 - macro_f1: 0.7532 - val_loss: 0.4028 - val_accuracy: 0.6575 - val_macro_f1: 0.6766 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.2035 - accuracy: 0.7249 - macro_f1: 0.7587 - val_loss: 0.3586 - val_accuracy: 0.6482 - val_macro_f1: 0.6775 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.1989 - accuracy: 0.7321 - macro_f1: 0.7657 - val_loss: 0.3865 - val_accuracy: 0.6757 - val_macro_f1: 0.6988 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.1921 - accuracy: 0.7352 - macro_f1: 0.7747 - val_loss: 0.3863 - val_accuracy: 0.6994 - val_macro_f1: 0.6853 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.1601 - accuracy: 0.7643 - macro_f1: 0.8139 - val_loss: 0.2795 - val_accuracy: 0.6906 - val_macro_f1: 0.7198 - lr: 1.0000e-04\n",
      "Epoch 12/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.1500 - accuracy: 0.7735 - macro_f1: 0.8255 - val_loss: 0.2934 - val_accuracy: 0.6966 - val_macro_f1: 0.7220 - lr: 1.0000e-04\n",
      "Epoch 13/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.1412 - accuracy: 0.7787 - macro_f1: 0.8370 - val_loss: 0.3077 - val_accuracy: 0.6971 - val_macro_f1: 0.7099 - lr: 1.0000e-04\n",
      "Epoch 14/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.1319 - accuracy: 0.7860 - macro_f1: 0.8491 - val_loss: 0.3250 - val_accuracy: 0.6971 - val_macro_f1: 0.7127 - lr: 1.0000e-04\n",
      "Epoch 15/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.1221 - accuracy: 0.7912 - macro_f1: 0.8568 - val_loss: 0.3584 - val_accuracy: 0.6985 - val_macro_f1: 0.6994 - lr: 1.0000e-04\n",
      "Epoch 16/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.1073 - accuracy: 0.7955 - macro_f1: 0.8770 - val_loss: 0.3691 - val_accuracy: 0.6892 - val_macro_f1: 0.7060 - lr: 1.0000e-04\n",
      "Epoch 17/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0825 - accuracy: 0.8149 - macro_f1: 0.9066 - val_loss: 0.3851 - val_accuracy: 0.6813 - val_macro_f1: 0.7065 - lr: 1.0000e-05\n",
      "Epoch 18/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0751 - accuracy: 0.8156 - macro_f1: 0.9165 - val_loss: 0.4284 - val_accuracy: 0.6799 - val_macro_f1: 0.7076 - lr: 1.0000e-05\n",
      "Epoch 19/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0697 - accuracy: 0.8191 - macro_f1: 0.9214 - val_loss: 0.4712 - val_accuracy: 0.6808 - val_macro_f1: 0.6969 - lr: 1.0000e-05\n",
      "Epoch 20/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0672 - accuracy: 0.8210 - macro_f1: 0.9255 - val_loss: 0.4898 - val_accuracy: 0.6766 - val_macro_f1: 0.6957 - lr: 1.0000e-05\n",
      "Epoch 21/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0633 - accuracy: 0.8222 - macro_f1: 0.9289 - val_loss: 0.5047 - val_accuracy: 0.6738 - val_macro_f1: 0.6958 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x73dc60d4cb50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_super_model = create_resnet_model(input_shape, num_classes_super)\n",
    "train_model(resnet_super_model, X_train, y_train_super, X_val, y_val_super, class_weight_super)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "267/267 [==============================] - 13s 20ms/step - loss: 0.3824 - accuracy: 0.4987 - macro_f1: 0.4738 - val_loss: 0.3772 - val_accuracy: 0.5718 - val_macro_f1: 0.5947 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.2926 - accuracy: 0.6282 - macro_f1: 0.6302 - val_loss: 0.3544 - val_accuracy: 0.6398 - val_macro_f1: 0.5986 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.2587 - accuracy: 0.6657 - macro_f1: 0.6803 - val_loss: 0.3343 - val_accuracy: 0.6654 - val_macro_f1: 0.6820 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.2377 - accuracy: 0.6905 - macro_f1: 0.7151 - val_loss: 0.3849 - val_accuracy: 0.6454 - val_macro_f1: 0.6654 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.2215 - accuracy: 0.7009 - macro_f1: 0.7318 - val_loss: 0.3230 - val_accuracy: 0.6827 - val_macro_f1: 0.6708 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.2084 - accuracy: 0.7177 - macro_f1: 0.7501 - val_loss: 0.3321 - val_accuracy: 0.6705 - val_macro_f1: 0.6537 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.1958 - accuracy: 0.7273 - macro_f1: 0.7657 - val_loss: 0.3942 - val_accuracy: 0.5979 - val_macro_f1: 0.6445 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.1811 - accuracy: 0.7389 - macro_f1: 0.7845 - val_loss: 0.3455 - val_accuracy: 0.6682 - val_macro_f1: 0.6650 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.1682 - accuracy: 0.7528 - macro_f1: 0.8031 - val_loss: 0.3625 - val_accuracy: 0.6692 - val_macro_f1: 0.6685 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.1560 - accuracy: 0.7582 - macro_f1: 0.8185 - val_loss: 0.3711 - val_accuracy: 0.6482 - val_macro_f1: 0.6737 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.1099 - accuracy: 0.7987 - macro_f1: 0.8785 - val_loss: 0.4045 - val_accuracy: 0.6780 - val_macro_f1: 0.6832 - lr: 1.0000e-04\n",
      "Epoch 12/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0939 - accuracy: 0.8097 - macro_f1: 0.8981 - val_loss: 0.4232 - val_accuracy: 0.6719 - val_macro_f1: 0.6842 - lr: 1.0000e-04\n",
      "Epoch 13/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0876 - accuracy: 0.8166 - macro_f1: 0.9032 - val_loss: 0.4394 - val_accuracy: 0.6696 - val_macro_f1: 0.6807 - lr: 1.0000e-04\n",
      "Epoch 14/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0817 - accuracy: 0.8217 - macro_f1: 0.9114 - val_loss: 0.4447 - val_accuracy: 0.6780 - val_macro_f1: 0.6820 - lr: 1.0000e-04\n",
      "Epoch 15/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0774 - accuracy: 0.8238 - macro_f1: 0.9167 - val_loss: 0.4665 - val_accuracy: 0.6673 - val_macro_f1: 0.6758 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x73dbe4067df0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_super_model = create_vit_model(input_shape, num_classes_super)\n",
    "train_model(vit_super_model, X_train, y_train_super, X_val, y_val_super, class_weight_super)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "267/267 [==============================] - 3s 7ms/step - loss: 0.1084 - accuracy: 0.3936 - macro_f1: 0.1386 - val_loss: 0.1403 - val_accuracy: 0.5303 - val_macro_f1: 0.1263 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0899 - accuracy: 0.4710 - macro_f1: 0.1888 - val_loss: 0.1414 - val_accuracy: 0.5065 - val_macro_f1: 0.1767 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0840 - accuracy: 0.4974 - macro_f1: 0.2213 - val_loss: 0.1536 - val_accuracy: 0.4623 - val_macro_f1: 0.1564 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0775 - accuracy: 0.5103 - macro_f1: 0.2367 - val_loss: 0.1536 - val_accuracy: 0.3975 - val_macro_f1: 0.2138 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0703 - accuracy: 0.5210 - macro_f1: 0.2643 - val_loss: 0.1270 - val_accuracy: 0.5163 - val_macro_f1: 0.2207 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0685 - accuracy: 0.5353 - macro_f1: 0.2711 - val_loss: 0.1256 - val_accuracy: 0.5410 - val_macro_f1: 0.2326 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0631 - accuracy: 0.5427 - macro_f1: 0.2945 - val_loss: 0.1241 - val_accuracy: 0.5377 - val_macro_f1: 0.2483 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0589 - accuracy: 0.5555 - macro_f1: 0.3134 - val_loss: 0.1118 - val_accuracy: 0.5722 - val_macro_f1: 0.2702 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0568 - accuracy: 0.5698 - macro_f1: 0.3240 - val_loss: 0.1112 - val_accuracy: 0.5760 - val_macro_f1: 0.2828 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0554 - accuracy: 0.5666 - macro_f1: 0.3287 - val_loss: 0.1210 - val_accuracy: 0.5485 - val_macro_f1: 0.2559 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0539 - accuracy: 0.5767 - macro_f1: 0.3356 - val_loss: 0.1128 - val_accuracy: 0.5871 - val_macro_f1: 0.2752 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0508 - accuracy: 0.5867 - macro_f1: 0.3482 - val_loss: 0.1149 - val_accuracy: 0.5764 - val_macro_f1: 0.2623 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0521 - accuracy: 0.5869 - macro_f1: 0.3492 - val_loss: 0.1141 - val_accuracy: 0.5718 - val_macro_f1: 0.2901 - lr: 0.0010\n",
      "Epoch 14/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0493 - accuracy: 0.5957 - macro_f1: 0.3662 - val_loss: 0.1120 - val_accuracy: 0.5825 - val_macro_f1: 0.2956 - lr: 0.0010\n",
      "Epoch 15/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0422 - accuracy: 0.6263 - macro_f1: 0.3865 - val_loss: 0.1048 - val_accuracy: 0.6048 - val_macro_f1: 0.3053 - lr: 1.0000e-04\n",
      "Epoch 16/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0406 - accuracy: 0.6319 - macro_f1: 0.3970 - val_loss: 0.1046 - val_accuracy: 0.6072 - val_macro_f1: 0.3072 - lr: 1.0000e-04\n",
      "Epoch 17/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0398 - accuracy: 0.6351 - macro_f1: 0.4045 - val_loss: 0.1054 - val_accuracy: 0.6039 - val_macro_f1: 0.3075 - lr: 1.0000e-04\n",
      "Epoch 18/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0391 - accuracy: 0.6330 - macro_f1: 0.4068 - val_loss: 0.1049 - val_accuracy: 0.6114 - val_macro_f1: 0.3069 - lr: 1.0000e-04\n",
      "Epoch 19/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0386 - accuracy: 0.6379 - macro_f1: 0.4086 - val_loss: 0.1052 - val_accuracy: 0.6109 - val_macro_f1: 0.3057 - lr: 1.0000e-04\n",
      "Epoch 20/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0381 - accuracy: 0.6391 - macro_f1: 0.4139 - val_loss: 0.1046 - val_accuracy: 0.6132 - val_macro_f1: 0.3122 - lr: 1.0000e-04\n",
      "Epoch 21/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0372 - accuracy: 0.6434 - macro_f1: 0.4156 - val_loss: 0.1051 - val_accuracy: 0.6081 - val_macro_f1: 0.3131 - lr: 1.0000e-04\n",
      "Epoch 22/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0364 - accuracy: 0.6470 - macro_f1: 0.4177 - val_loss: 0.1043 - val_accuracy: 0.6072 - val_macro_f1: 0.3130 - lr: 1.0000e-05\n",
      "Epoch 23/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0360 - accuracy: 0.6465 - macro_f1: 0.4243 - val_loss: 0.1049 - val_accuracy: 0.6090 - val_macro_f1: 0.3145 - lr: 1.0000e-05\n",
      "Epoch 24/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0362 - accuracy: 0.6480 - macro_f1: 0.4250 - val_loss: 0.1045 - val_accuracy: 0.6081 - val_macro_f1: 0.3172 - lr: 1.0000e-05\n",
      "Epoch 25/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0361 - accuracy: 0.6496 - macro_f1: 0.4252 - val_loss: 0.1052 - val_accuracy: 0.6104 - val_macro_f1: 0.3076 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x73dbccd2f070>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_sub = y_train_sub.shape[1]\n",
    "cnn_sub_model = create_cnn_model(input_shape, num_classes_sub)\n",
    "train_model(cnn_sub_model, X_train, y_train_sub, X_val, y_val_sub, class_weight_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "267/267 [==============================] - 14s 21ms/step - loss: 0.1181 - accuracy: 0.3439 - macro_f1: 0.0673 - val_loss: 0.1689 - val_accuracy: 0.4264 - val_macro_f1: 0.1547 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.1026 - accuracy: 0.4255 - macro_f1: 0.1100 - val_loss: 0.3509 - val_accuracy: 0.0974 - val_macro_f1: 0.0624 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0921 - accuracy: 0.4473 - macro_f1: 0.1355 - val_loss: 0.1577 - val_accuracy: 0.4567 - val_macro_f1: 0.1011 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0856 - accuracy: 0.4694 - macro_f1: 0.1732 - val_loss: 0.1486 - val_accuracy: 0.5140 - val_macro_f1: 0.1834 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0831 - accuracy: 0.4777 - macro_f1: 0.1829 - val_loss: 0.1678 - val_accuracy: 0.3551 - val_macro_f1: 0.1285 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0811 - accuracy: 0.4707 - macro_f1: 0.1861 - val_loss: 0.1917 - val_accuracy: 0.5121 - val_macro_f1: 0.1613 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0778 - accuracy: 0.4817 - macro_f1: 0.2111 - val_loss: 0.2446 - val_accuracy: 0.1263 - val_macro_f1: 0.0802 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0728 - accuracy: 0.4934 - macro_f1: 0.2308 - val_loss: 0.1729 - val_accuracy: 0.3751 - val_macro_f1: 0.1667 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0713 - accuracy: 0.5077 - macro_f1: 0.2412 - val_loss: 0.1585 - val_accuracy: 0.3961 - val_macro_f1: 0.1645 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0626 - accuracy: 0.5385 - macro_f1: 0.2696 - val_loss: 0.1184 - val_accuracy: 0.5531 - val_macro_f1: 0.2544 - lr: 1.0000e-04\n",
      "Epoch 11/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0583 - accuracy: 0.5467 - macro_f1: 0.2953 - val_loss: 0.1167 - val_accuracy: 0.5559 - val_macro_f1: 0.2734 - lr: 1.0000e-04\n",
      "Epoch 12/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0561 - accuracy: 0.5580 - macro_f1: 0.3090 - val_loss: 0.1166 - val_accuracy: 0.5494 - val_macro_f1: 0.2833 - lr: 1.0000e-04\n",
      "Epoch 13/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0547 - accuracy: 0.5554 - macro_f1: 0.3171 - val_loss: 0.1143 - val_accuracy: 0.5629 - val_macro_f1: 0.2820 - lr: 1.0000e-04\n",
      "Epoch 14/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0531 - accuracy: 0.5581 - macro_f1: 0.3258 - val_loss: 0.1146 - val_accuracy: 0.5615 - val_macro_f1: 0.2866 - lr: 1.0000e-04\n",
      "Epoch 15/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0514 - accuracy: 0.5691 - macro_f1: 0.3357 - val_loss: 0.1166 - val_accuracy: 0.5573 - val_macro_f1: 0.2922 - lr: 1.0000e-04\n",
      "Epoch 16/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0501 - accuracy: 0.5722 - macro_f1: 0.3408 - val_loss: 0.1120 - val_accuracy: 0.5713 - val_macro_f1: 0.3001 - lr: 1.0000e-04\n",
      "Epoch 17/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0488 - accuracy: 0.5812 - macro_f1: 0.3516 - val_loss: 0.1122 - val_accuracy: 0.5778 - val_macro_f1: 0.3055 - lr: 1.0000e-04\n",
      "Epoch 18/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0475 - accuracy: 0.5866 - macro_f1: 0.3606 - val_loss: 0.1134 - val_accuracy: 0.5704 - val_macro_f1: 0.3037 - lr: 1.0000e-04\n",
      "Epoch 19/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0461 - accuracy: 0.5887 - macro_f1: 0.3637 - val_loss: 0.1130 - val_accuracy: 0.5732 - val_macro_f1: 0.3085 - lr: 1.0000e-04\n",
      "Epoch 20/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0443 - accuracy: 0.5963 - macro_f1: 0.3746 - val_loss: 0.1139 - val_accuracy: 0.5806 - val_macro_f1: 0.3023 - lr: 1.0000e-04\n",
      "Epoch 21/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0428 - accuracy: 0.6013 - macro_f1: 0.3855 - val_loss: 0.1180 - val_accuracy: 0.5499 - val_macro_f1: 0.3122 - lr: 1.0000e-04\n",
      "Epoch 22/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0401 - accuracy: 0.6064 - macro_f1: 0.3940 - val_loss: 0.1131 - val_accuracy: 0.5750 - val_macro_f1: 0.3154 - lr: 1.0000e-05\n",
      "Epoch 23/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0393 - accuracy: 0.6152 - macro_f1: 0.4011 - val_loss: 0.1122 - val_accuracy: 0.5825 - val_macro_f1: 0.3187 - lr: 1.0000e-05\n",
      "Epoch 24/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0388 - accuracy: 0.6164 - macro_f1: 0.4052 - val_loss: 0.1127 - val_accuracy: 0.5829 - val_macro_f1: 0.3177 - lr: 1.0000e-05\n",
      "Epoch 25/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0385 - accuracy: 0.6204 - macro_f1: 0.4089 - val_loss: 0.1124 - val_accuracy: 0.5811 - val_macro_f1: 0.3164 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x73dbc61b7d90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_sub_model = create_resnet_model(input_shape, num_classes_sub)\n",
    "train_model(resnet_sub_model, X_train, y_train_sub, X_val, y_val_sub, class_weight_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "267/267 [==============================] - 13s 20ms/step - loss: 0.1335 - accuracy: 0.2333 - macro_f1: 0.0649 - val_loss: 0.1869 - val_accuracy: 0.2773 - val_macro_f1: 0.0707 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0894 - accuracy: 0.4011 - macro_f1: 0.1432 - val_loss: 0.1557 - val_accuracy: 0.4357 - val_macro_f1: 0.1488 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0701 - accuracy: 0.4798 - macro_f1: 0.2268 - val_loss: 0.1522 - val_accuracy: 0.4543 - val_macro_f1: 0.1872 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0587 - accuracy: 0.5259 - macro_f1: 0.2836 - val_loss: 0.1545 - val_accuracy: 0.4278 - val_macro_f1: 0.1870 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0489 - accuracy: 0.5568 - macro_f1: 0.3374 - val_loss: 0.1516 - val_accuracy: 0.5037 - val_macro_f1: 0.2110 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0433 - accuracy: 0.5870 - macro_f1: 0.3787 - val_loss: 0.1547 - val_accuracy: 0.4818 - val_macro_f1: 0.2275 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0377 - accuracy: 0.6145 - macro_f1: 0.4094 - val_loss: 0.1472 - val_accuracy: 0.5284 - val_macro_f1: 0.2131 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0342 - accuracy: 0.6213 - macro_f1: 0.4374 - val_loss: 0.1476 - val_accuracy: 0.5447 - val_macro_f1: 0.2437 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0304 - accuracy: 0.6483 - macro_f1: 0.4640 - val_loss: 0.1577 - val_accuracy: 0.5051 - val_macro_f1: 0.2365 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0268 - accuracy: 0.6679 - macro_f1: 0.4866 - val_loss: 0.1620 - val_accuracy: 0.5112 - val_macro_f1: 0.2333 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0241 - accuracy: 0.6821 - macro_f1: 0.5074 - val_loss: 0.1630 - val_accuracy: 0.5298 - val_macro_f1: 0.2370 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0230 - accuracy: 0.6864 - macro_f1: 0.5213 - val_loss: 0.1729 - val_accuracy: 0.5121 - val_macro_f1: 0.2505 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0193 - accuracy: 0.7166 - macro_f1: 0.5422 - val_loss: 0.1639 - val_accuracy: 0.5648 - val_macro_f1: 0.2586 - lr: 1.0000e-04\n",
      "Epoch 14/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0155 - accuracy: 0.7324 - macro_f1: 0.5754 - val_loss: 0.1667 - val_accuracy: 0.5610 - val_macro_f1: 0.2593 - lr: 1.0000e-04\n",
      "Epoch 15/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0138 - accuracy: 0.7444 - macro_f1: 0.5898 - val_loss: 0.1701 - val_accuracy: 0.5606 - val_macro_f1: 0.2612 - lr: 1.0000e-04\n",
      "Epoch 16/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0127 - accuracy: 0.7536 - macro_f1: 0.5964 - val_loss: 0.1726 - val_accuracy: 0.5555 - val_macro_f1: 0.2631 - lr: 1.0000e-04\n",
      "Epoch 17/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0120 - accuracy: 0.7586 - macro_f1: 0.6087 - val_loss: 0.1739 - val_accuracy: 0.5666 - val_macro_f1: 0.2631 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x73db45152260>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_sub_model = create_vit_model(input_shape, num_classes_sub)\n",
    "train_model(vit_sub_model, X_train, y_train_sub, X_val, y_val_sub, class_weight_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, classes):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_threshold = (y_pred >= 0.5).astype(int)\n",
    "    report = classification_report(y_test, y_pred_threshold, target_names=classes, zero_division=0, output_dict=True)\n",
    "    print(classification_report(y_test, y_pred_threshold, target_names=classes, zero_division=0))\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Superdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.84      0.68      0.75       496\n",
      "         HYP       0.56      0.65      0.60       262\n",
      "          MI       0.84      0.61      0.71       550\n",
      "        NORM       0.83      0.88      0.86       963\n",
      "        STTC       0.80      0.71      0.75       521\n",
      "\n",
      "   micro avg       0.80      0.74      0.77      2792\n",
      "   macro avg       0.77      0.71      0.73      2792\n",
      "weighted avg       0.80      0.74      0.76      2792\n",
      " samples avg       0.77      0.76      0.75      2792\n",
      "\n",
      "ResNet Superdiagnostic Classification Report:\n",
      "68/68 [==============================] - 1s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.81      0.69      0.75       496\n",
      "         HYP       0.72      0.50      0.59       262\n",
      "          MI       0.72      0.77      0.74       550\n",
      "        NORM       0.87      0.82      0.84       963\n",
      "        STTC       0.72      0.78      0.75       521\n",
      "\n",
      "   micro avg       0.78      0.75      0.77      2792\n",
      "   macro avg       0.77      0.71      0.73      2792\n",
      "weighted avg       0.79      0.75      0.76      2792\n",
      " samples avg       0.77      0.77      0.75      2792\n",
      "\n",
      "ViT Superdiagnostic Classification Report:\n",
      "68/68 [==============================] - 1s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.75      0.63      0.69       496\n",
      "         HYP       0.68      0.43      0.53       262\n",
      "          MI       0.76      0.54      0.63       550\n",
      "        NORM       0.78      0.88      0.83       963\n",
      "        STTC       0.73      0.66      0.69       521\n",
      "\n",
      "   micro avg       0.76      0.68      0.72      2792\n",
      "   macro avg       0.74      0.63      0.67      2792\n",
      "weighted avg       0.75      0.68      0.71      2792\n",
      " samples avg       0.74      0.72      0.71      2792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN Superdiagnostic Classification Report:\")\n",
    "cnn_super_report = evaluate_model(cnn_super_model, X_test, y_test_super, classes_super)\n",
    "\n",
    "print(\"ResNet Superdiagnostic Classification Report:\")\n",
    "resnet_super_report = evaluate_model(resnet_super_model, X_test, y_test_super, classes_super)\n",
    "\n",
    "print(\"ViT Superdiagnostic Classification Report:\")\n",
    "vit_super_report = evaluate_model(vit_super_model, X_test, y_test_super, classes_super)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Subdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.88      0.53      0.66       306\n",
      "       CLBBB       0.91      0.91      0.91        54\n",
      "       CRBBB       0.79      0.91      0.84        54\n",
      "       ILBBB       0.12      0.12      0.12         8\n",
      "         IMI       0.79      0.48      0.60       327\n",
      "       IRBBB       0.56      0.68      0.62       112\n",
      "        ISCA       0.54      0.16      0.25        93\n",
      "        ISCI       0.44      0.30      0.36        40\n",
      "        ISC_       0.72      0.47      0.57       128\n",
      "        IVCD       0.19      0.06      0.09        79\n",
      "   LAFB/LPFB       0.80      0.64      0.71       179\n",
      "     LAO/LAE       0.00      0.00      0.00        42\n",
      "         LMI       0.30      0.15      0.20        20\n",
      "         LVH       0.75      0.53      0.62       214\n",
      "        NORM       0.88      0.76      0.81       963\n",
      "        NST_       0.32      0.16      0.21        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.57      0.40      0.47        10\n",
      "         RVH       1.00      0.08      0.15        12\n",
      "       SEHYP       0.00      0.00      0.00         2\n",
      "        STTC       0.52      0.38      0.44       222\n",
      "         WPW       0.83      0.62      0.71         8\n",
      "        _AVB       0.53      0.21      0.30        82\n",
      "\n",
      "   micro avg       0.76      0.55      0.64      3034\n",
      "   macro avg       0.54      0.37      0.42      3034\n",
      "weighted avg       0.73      0.55      0.62      3034\n",
      " samples avg       0.62      0.59      0.59      3034\n",
      "\n",
      "ResNet Subdiagnostic Classification Report:\n",
      "68/68 [==============================] - 1s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.77      0.40      0.52       306\n",
      "       CLBBB       0.96      0.91      0.93        54\n",
      "       CRBBB       0.80      0.94      0.86        54\n",
      "       ILBBB       0.11      0.12      0.12         8\n",
      "         IMI       0.71      0.43      0.53       327\n",
      "       IRBBB       0.47      0.68      0.55       112\n",
      "        ISCA       0.33      0.18      0.23        93\n",
      "        ISCI       0.50      0.35      0.41        40\n",
      "        ISC_       0.75      0.52      0.61       128\n",
      "        IVCD       0.14      0.03      0.04        79\n",
      "   LAFB/LPFB       0.80      0.73      0.76       179\n",
      "     LAO/LAE       0.00      0.00      0.00        42\n",
      "         LMI       0.08      0.05      0.06        20\n",
      "         LVH       0.73      0.52      0.60       214\n",
      "        NORM       0.87      0.71      0.78       963\n",
      "        NST_       0.24      0.18      0.21        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.40      0.40      0.40        10\n",
      "         RVH       0.17      0.08      0.11        12\n",
      "       SEHYP       0.00      0.00      0.00         2\n",
      "        STTC       0.51      0.35      0.41       222\n",
      "         WPW       1.00      0.50      0.67         8\n",
      "        _AVB       0.56      0.28      0.37        82\n",
      "\n",
      "   micro avg       0.72      0.52      0.60      3034\n",
      "   macro avg       0.47      0.36      0.40      3034\n",
      "weighted avg       0.70      0.52      0.59      3034\n",
      " samples avg       0.59      0.56      0.56      3034\n",
      "\n",
      "ViT Subdiagnostic Classification Report:\n",
      "68/68 [==============================] - 1s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.70      0.42      0.53       306\n",
      "       CLBBB       0.88      0.85      0.87        54\n",
      "       CRBBB       0.86      0.81      0.84        54\n",
      "       ILBBB       0.00      0.00      0.00         8\n",
      "         IMI       0.65      0.46      0.54       327\n",
      "       IRBBB       0.56      0.27      0.36       112\n",
      "        ISCA       0.38      0.05      0.09        93\n",
      "        ISCI       0.20      0.03      0.04        40\n",
      "        ISC_       0.63      0.38      0.48       128\n",
      "        IVCD       0.06      0.06      0.06        79\n",
      "   LAFB/LPFB       0.83      0.36      0.50       179\n",
      "     LAO/LAE       0.50      0.02      0.05        42\n",
      "         LMI       0.25      0.05      0.08        20\n",
      "         LVH       0.61      0.39      0.48       214\n",
      "        NORM       0.85      0.62      0.72       963\n",
      "        NST_       0.10      0.12      0.11        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.00      0.00      0.00        10\n",
      "         RVH       0.14      0.08      0.11        12\n",
      "       SEHYP       0.00      0.00      0.00         2\n",
      "        STTC       0.39      0.25      0.31       222\n",
      "         WPW       1.00      0.12      0.22         8\n",
      "        _AVB       0.13      0.04      0.06        82\n",
      "\n",
      "   micro avg       0.66      0.42      0.51      3034\n",
      "   macro avg       0.42      0.23      0.28      3034\n",
      "weighted avg       0.64      0.42      0.50      3034\n",
      " samples avg       0.48      0.46      0.45      3034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN Subdiagnostic Classification Report:\")\n",
    "cnn_sub_report = evaluate_model(cnn_sub_model, X_test, y_test_sub, classes_sub)\n",
    "\n",
    "print(\"ResNet Subdiagnostic Classification Report:\")\n",
    "resnet_sub_report = evaluate_model(resnet_sub_model, X_test, y_test_sub, classes_sub)\n",
    "\n",
    "print(\"ViT Subdiagnostic Classification Report:\")\n",
    "vit_sub_report = evaluate_model(vit_sub_model, X_test, y_test_sub, classes_sub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining and Training on LwF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534/534 [==============================] - 1s 1ms/step\n",
      "Working on CNN for LwF Now:\n",
      "Epoch 1/25\n",
      "267/267 [==============================] - 3s 7ms/step - loss: 0.1081 - accuracy: 0.4157 - macro_f1: 0.1335 - val_loss: 0.1729 - val_accuracy: 0.3565 - val_macro_f1: 0.1212 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0866 - accuracy: 0.4786 - macro_f1: 0.1963 - val_loss: 0.1272 - val_accuracy: 0.5536 - val_macro_f1: 0.1921 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0817 - accuracy: 0.4922 - macro_f1: 0.2250 - val_loss: 0.1319 - val_accuracy: 0.5238 - val_macro_f1: 0.2143 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0743 - accuracy: 0.5280 - macro_f1: 0.2534 - val_loss: 0.1643 - val_accuracy: 0.3723 - val_macro_f1: 0.1882 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0695 - accuracy: 0.5175 - macro_f1: 0.2734 - val_loss: 0.1398 - val_accuracy: 0.4278 - val_macro_f1: 0.2200 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0641 - accuracy: 0.5360 - macro_f1: 0.2955 - val_loss: 0.1293 - val_accuracy: 0.5079 - val_macro_f1: 0.2058 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0600 - accuracy: 0.5571 - macro_f1: 0.3053 - val_loss: 0.1166 - val_accuracy: 0.5727 - val_macro_f1: 0.2529 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0583 - accuracy: 0.5602 - macro_f1: 0.3157 - val_loss: 0.1152 - val_accuracy: 0.6025 - val_macro_f1: 0.2535 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0573 - accuracy: 0.5598 - macro_f1: 0.3230 - val_loss: 0.1200 - val_accuracy: 0.5200 - val_macro_f1: 0.2767 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0546 - accuracy: 0.5802 - macro_f1: 0.3370 - val_loss: 0.1101 - val_accuracy: 0.5755 - val_macro_f1: 0.2943 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0515 - accuracy: 0.5952 - macro_f1: 0.3527 - val_loss: 0.1118 - val_accuracy: 0.5788 - val_macro_f1: 0.2914 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0522 - accuracy: 0.5845 - macro_f1: 0.3487 - val_loss: 0.1118 - val_accuracy: 0.5979 - val_macro_f1: 0.2875 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0516 - accuracy: 0.5925 - macro_f1: 0.3491 - val_loss: 0.1124 - val_accuracy: 0.5829 - val_macro_f1: 0.2920 - lr: 0.0010\n",
      "Epoch 14/25\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0473 - accuracy: 0.6055 - macro_f1: 0.3692 - val_loss: 0.1058 - val_accuracy: 0.6114 - val_macro_f1: 0.2908 - lr: 0.0010\n",
      "Epoch 15/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0465 - accuracy: 0.6080 - macro_f1: 0.3739 - val_loss: 0.1093 - val_accuracy: 0.6030 - val_macro_f1: 0.3033 - lr: 0.0010\n",
      "Epoch 16/25\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0454 - accuracy: 0.6085 - macro_f1: 0.3780 - val_loss: 0.1084 - val_accuracy: 0.5983 - val_macro_f1: 0.3044 - lr: 0.0010\n",
      "Epoch 17/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0432 - accuracy: 0.6213 - macro_f1: 0.3969 - val_loss: 0.1084 - val_accuracy: 0.5979 - val_macro_f1: 0.3059 - lr: 0.0010\n",
      "Epoch 18/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0415 - accuracy: 0.6198 - macro_f1: 0.3945 - val_loss: 0.1135 - val_accuracy: 0.5951 - val_macro_f1: 0.3058 - lr: 0.0010\n",
      "Epoch 19/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0389 - accuracy: 0.6322 - macro_f1: 0.4141 - val_loss: 0.1135 - val_accuracy: 0.5811 - val_macro_f1: 0.2999 - lr: 0.0010\n",
      "Epoch 20/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0342 - accuracy: 0.6542 - macro_f1: 0.4406 - val_loss: 0.1038 - val_accuracy: 0.6132 - val_macro_f1: 0.3274 - lr: 1.0000e-04\n",
      "Epoch 21/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0318 - accuracy: 0.6619 - macro_f1: 0.4539 - val_loss: 0.1032 - val_accuracy: 0.6160 - val_macro_f1: 0.3262 - lr: 1.0000e-04\n",
      "Epoch 22/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0310 - accuracy: 0.6694 - macro_f1: 0.4557 - val_loss: 0.1045 - val_accuracy: 0.6095 - val_macro_f1: 0.3320 - lr: 1.0000e-04\n",
      "Epoch 23/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0302 - accuracy: 0.6673 - macro_f1: 0.4642 - val_loss: 0.1038 - val_accuracy: 0.6174 - val_macro_f1: 0.3305 - lr: 1.0000e-04\n",
      "Epoch 24/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0297 - accuracy: 0.6730 - macro_f1: 0.4672 - val_loss: 0.1052 - val_accuracy: 0.6170 - val_macro_f1: 0.3348 - lr: 1.0000e-04\n",
      "Epoch 25/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0291 - accuracy: 0.6724 - macro_f1: 0.4771 - val_loss: 0.1057 - val_accuracy: 0.6165 - val_macro_f1: 0.3306 - lr: 1.0000e-04\n",
      "Working on ResNet for LwF Now:\n",
      "534/534 [==============================] - 2s 3ms/step\n",
      "Epoch 1/25\n",
      "267/267 [==============================] - 14s 21ms/step - loss: 0.1138 - accuracy: 0.3607 - macro_f1: 0.0761 - val_loss: 0.2215 - val_accuracy: 0.2651 - val_macro_f1: 0.0926 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0955 - accuracy: 0.4418 - macro_f1: 0.1303 - val_loss: 0.3504 - val_accuracy: 0.1514 - val_macro_f1: 0.0680 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0904 - accuracy: 0.4718 - macro_f1: 0.1547 - val_loss: 0.1965 - val_accuracy: 0.4129 - val_macro_f1: 0.1039 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0875 - accuracy: 0.4756 - macro_f1: 0.1697 - val_loss: 0.2209 - val_accuracy: 0.3243 - val_macro_f1: 0.1251 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0850 - accuracy: 0.4797 - macro_f1: 0.1887 - val_loss: 0.1625 - val_accuracy: 0.3900 - val_macro_f1: 0.1456 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0785 - accuracy: 0.4847 - macro_f1: 0.2031 - val_loss: 0.1492 - val_accuracy: 0.4222 - val_macro_f1: 0.1998 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0759 - accuracy: 0.4985 - macro_f1: 0.2218 - val_loss: 0.1647 - val_accuracy: 0.5065 - val_macro_f1: 0.2033 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0709 - accuracy: 0.5130 - macro_f1: 0.2467 - val_loss: 0.1353 - val_accuracy: 0.4856 - val_macro_f1: 0.2146 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0756 - accuracy: 0.5091 - macro_f1: 0.2591 - val_loss: 0.3255 - val_accuracy: 0.4273 - val_macro_f1: 0.1915 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0676 - accuracy: 0.5139 - macro_f1: 0.2648 - val_loss: 0.1385 - val_accuracy: 0.5135 - val_macro_f1: 0.2276 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0618 - accuracy: 0.5362 - macro_f1: 0.2881 - val_loss: 0.1423 - val_accuracy: 0.4539 - val_macro_f1: 0.2408 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0591 - accuracy: 0.5489 - macro_f1: 0.3091 - val_loss: 0.1674 - val_accuracy: 0.4199 - val_macro_f1: 0.2237 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0567 - accuracy: 0.5680 - macro_f1: 0.3185 - val_loss: 0.1671 - val_accuracy: 0.4045 - val_macro_f1: 0.2123 - lr: 0.0010\n",
      "Epoch 14/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0499 - accuracy: 0.5860 - macro_f1: 0.3468 - val_loss: 0.1106 - val_accuracy: 0.5862 - val_macro_f1: 0.2972 - lr: 1.0000e-04\n",
      "Epoch 15/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0459 - accuracy: 0.6029 - macro_f1: 0.3720 - val_loss: 0.1097 - val_accuracy: 0.5881 - val_macro_f1: 0.3061 - lr: 1.0000e-04\n",
      "Epoch 16/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0445 - accuracy: 0.6082 - macro_f1: 0.3802 - val_loss: 0.1103 - val_accuracy: 0.5843 - val_macro_f1: 0.3124 - lr: 1.0000e-04\n",
      "Epoch 17/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0434 - accuracy: 0.6133 - macro_f1: 0.3875 - val_loss: 0.1125 - val_accuracy: 0.5760 - val_macro_f1: 0.3124 - lr: 1.0000e-04\n",
      "Epoch 18/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0425 - accuracy: 0.6145 - macro_f1: 0.3867 - val_loss: 0.1102 - val_accuracy: 0.6021 - val_macro_f1: 0.3104 - lr: 1.0000e-04\n",
      "Epoch 19/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0414 - accuracy: 0.6211 - macro_f1: 0.3933 - val_loss: 0.1121 - val_accuracy: 0.6002 - val_macro_f1: 0.3075 - lr: 1.0000e-04\n",
      "Epoch 20/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0403 - accuracy: 0.6228 - macro_f1: 0.4036 - val_loss: 0.1105 - val_accuracy: 0.5876 - val_macro_f1: 0.3131 - lr: 1.0000e-04\n",
      "Epoch 21/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0381 - accuracy: 0.6291 - macro_f1: 0.4079 - val_loss: 0.1095 - val_accuracy: 0.5941 - val_macro_f1: 0.3187 - lr: 1.0000e-05\n",
      "Epoch 22/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0375 - accuracy: 0.6304 - macro_f1: 0.4177 - val_loss: 0.1102 - val_accuracy: 0.5946 - val_macro_f1: 0.3188 - lr: 1.0000e-05\n",
      "Epoch 23/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0373 - accuracy: 0.6356 - macro_f1: 0.4182 - val_loss: 0.1102 - val_accuracy: 0.5960 - val_macro_f1: 0.3224 - lr: 1.0000e-05\n",
      "Epoch 24/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0371 - accuracy: 0.6372 - macro_f1: 0.4201 - val_loss: 0.1109 - val_accuracy: 0.5937 - val_macro_f1: 0.3219 - lr: 1.0000e-05\n",
      "Epoch 25/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0370 - accuracy: 0.6322 - macro_f1: 0.4183 - val_loss: 0.1110 - val_accuracy: 0.5937 - val_macro_f1: 0.3235 - lr: 1.0000e-05\n",
      "Working on ViT for LwF Now:\n",
      "534/534 [==============================] - 3s 6ms/step\n",
      "Epoch 1/25\n",
      "267/267 [==============================] - 13s 21ms/step - loss: 0.1330 - accuracy: 0.2103 - macro_f1: 0.0625 - val_loss: 0.1793 - val_accuracy: 0.3122 - val_macro_f1: 0.0855 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0930 - accuracy: 0.3858 - macro_f1: 0.1318 - val_loss: 0.1750 - val_accuracy: 0.4143 - val_macro_f1: 0.1146 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0725 - accuracy: 0.4639 - macro_f1: 0.2137 - val_loss: 0.1462 - val_accuracy: 0.5256 - val_macro_f1: 0.1697 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0580 - accuracy: 0.5310 - macro_f1: 0.2875 - val_loss: 0.1514 - val_accuracy: 0.4944 - val_macro_f1: 0.1935 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0492 - accuracy: 0.5612 - macro_f1: 0.3339 - val_loss: 0.1565 - val_accuracy: 0.4334 - val_macro_f1: 0.2084 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0429 - accuracy: 0.5933 - macro_f1: 0.3796 - val_loss: 0.1444 - val_accuracy: 0.5349 - val_macro_f1: 0.2166 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0379 - accuracy: 0.6105 - macro_f1: 0.4103 - val_loss: 0.1508 - val_accuracy: 0.5098 - val_macro_f1: 0.2164 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0379 - accuracy: 0.6218 - macro_f1: 0.4209 - val_loss: 0.1494 - val_accuracy: 0.5219 - val_macro_f1: 0.2270 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0331 - accuracy: 0.6382 - macro_f1: 0.4436 - val_loss: 0.1574 - val_accuracy: 0.5186 - val_macro_f1: 0.2457 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0280 - accuracy: 0.6659 - macro_f1: 0.4841 - val_loss: 0.1578 - val_accuracy: 0.5541 - val_macro_f1: 0.2465 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0253 - accuracy: 0.6836 - macro_f1: 0.5034 - val_loss: 0.1614 - val_accuracy: 0.5531 - val_macro_f1: 0.2287 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0182 - accuracy: 0.7255 - macro_f1: 0.5507 - val_loss: 0.1553 - val_accuracy: 0.5676 - val_macro_f1: 0.2572 - lr: 1.0000e-04\n",
      "Epoch 13/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0158 - accuracy: 0.7362 - macro_f1: 0.5669 - val_loss: 0.1600 - val_accuracy: 0.5680 - val_macro_f1: 0.2552 - lr: 1.0000e-04\n",
      "Epoch 14/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0146 - accuracy: 0.7410 - macro_f1: 0.5790 - val_loss: 0.1614 - val_accuracy: 0.5671 - val_macro_f1: 0.2581 - lr: 1.0000e-04\n",
      "Epoch 15/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0139 - accuracy: 0.7450 - macro_f1: 0.5852 - val_loss: 0.1617 - val_accuracy: 0.5671 - val_macro_f1: 0.2601 - lr: 1.0000e-04\n",
      "Epoch 16/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0133 - accuracy: 0.7516 - macro_f1: 0.5905 - val_loss: 0.1646 - val_accuracy: 0.5629 - val_macro_f1: 0.2585 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x73db12ace5c0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_soft_targets_super = cnn_super_model.predict(X_train)\n",
    "\n",
    "def lwf_loss(y_true, y_pred, old_predictions, T=2):\n",
    "    task_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    dist_loss = tf.keras.losses.KLDivergence()(tf.nn.softmax(old_predictions / T),\n",
    "                                               tf.nn.softmax(y_pred / T))\n",
    "    total_loss = task_loss + dist_loss\n",
    "    return total_loss\n",
    "\n",
    "print(\"Working on CNN for LwF Now:\")\n",
    "cnn_model_lwf = create_cnn_model(input_shape, num_classes_sub)\n",
    "cnn_model_lwf.compile(\n",
    "    optimizer='adam',\n",
    "    loss=lambda y_true, y_pred: lwf_loss(y_true, y_pred, old_predictions=cnn_soft_targets_super),\n",
    "    metrics=[macro_f1]\n",
    ")\n",
    "train_model(cnn_model_lwf, X_train, y_train_sub, X_val, y_val_sub, class_weight_sub)\n",
    "\n",
    "print(\"Working on ResNet for LwF Now:\")\n",
    "resnet_soft_targets_super = resnet_super_model.predict(X_train)\n",
    "resnet_model_lwf = create_resnet_model(input_shape, num_classes_sub)\n",
    "resnet_model_lwf.compile(\n",
    "    optimizer='adam',\n",
    "    loss=lambda y_true, y_pred: lwf_loss(y_true, y_pred, old_predictions=resnet_soft_targets_super),\n",
    "    metrics=[macro_f1]\n",
    ")\n",
    "train_model(resnet_model_lwf, X_train, y_train_sub, X_val, y_val_sub, class_weight_sub)\n",
    "\n",
    "print(\"Working on ViT for LwF Now:\")\n",
    "vit_soft_targets_super = vit_super_model.predict(X_train)\n",
    "vit_model_lwf = create_vit_model(input_shape, num_classes_sub)\n",
    "vit_model_lwf.compile(\n",
    "    optimizer='adam',\n",
    "    loss=lambda y_true, y_pred: lwf_loss(y_true, y_pred, old_predictions=vit_soft_targets_super),\n",
    "    metrics=[macro_f1]\n",
    ")\n",
    "train_model(vit_model_lwf, X_train, y_train_sub, X_val, y_val_sub, class_weight_sub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining and Training on EwC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EWC:\n",
    "    def __init__(self, model, X, y, batch_size=32, exclude_params=[]):\n",
    "        self.model = model\n",
    "        self.params = {}\n",
    "        for p in model.trainable_variables:\n",
    "            if id(p) not in exclude_params:\n",
    "                self.params[id(p)] = p.numpy()\n",
    "        self.fisher = self.compute_fisher(X, y, batch_size, exclude_params)\n",
    "\n",
    "    def compute_fisher(self, X, y, batch_size, exclude_params):\n",
    "        fisher = {}\n",
    "        num_samples = X.shape[0]\n",
    "        num_batches = int(np.ceil(num_samples / batch_size))\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            X_batch = X[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "            y_batch = y[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "            with tf.GradientTape() as tape:\n",
    "                preds = self.model(X_batch)\n",
    "                loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_batch, preds))\n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            for p, g in zip(self.model.trainable_variables, grads):\n",
    "                if g is not None and id(p) not in exclude_params:\n",
    "                    param_id = id(p)\n",
    "                    if param_id not in fisher:\n",
    "                        fisher[param_id] = np.square(g.numpy())\n",
    "                    else:\n",
    "                        fisher[param_id] += np.square(g.numpy())\n",
    "        for k in fisher.keys():\n",
    "            fisher[k] /= num_batches\n",
    "        return fisher\n",
    "\n",
    "    def penalty(self, model):\n",
    "        loss = 0\n",
    "        for p in model.trainable_variables:\n",
    "            param_id = id(p)\n",
    "            if param_id in self.fisher:\n",
    "                fisher = tf.convert_to_tensor(self.fisher[param_id])\n",
    "                loss += tf.reduce_sum(fisher * tf.square(p - self.params[param_id]))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_model_for_subdiagnostic(base_model, num_classes_sub):\n",
    "    x = base_model.layers[-2].output\n",
    "    outputs = layers.Dense(num_classes_sub, activation='sigmoid', name='output_sub')(x)\n",
    "    new_model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "    return new_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "267/267 [==============================] - 3s 7ms/step - loss: 0.0898 - accuracy: 0.5516 - macro_f1: 0.2330 - val_loss: 0.1200 - val_accuracy: 0.5727 - val_macro_f1: 0.2476 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0689 - accuracy: 0.5959 - macro_f1: 0.2996 - val_loss: 0.1143 - val_accuracy: 0.5708 - val_macro_f1: 0.2700 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0612 - accuracy: 0.5940 - macro_f1: 0.3263 - val_loss: 0.1408 - val_accuracy: 0.4478 - val_macro_f1: 0.2596 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0589 - accuracy: 0.6023 - macro_f1: 0.3273 - val_loss: 0.1155 - val_accuracy: 0.5727 - val_macro_f1: 0.2822 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0562 - accuracy: 0.6028 - macro_f1: 0.3462 - val_loss: 0.1235 - val_accuracy: 0.5634 - val_macro_f1: 0.2811 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0514 - accuracy: 0.6164 - macro_f1: 0.3614 - val_loss: 0.1103 - val_accuracy: 0.6086 - val_macro_f1: 0.2905 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0483 - accuracy: 0.6223 - macro_f1: 0.3732 - val_loss: 0.1083 - val_accuracy: 0.6142 - val_macro_f1: 0.3168 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0474 - accuracy: 0.6220 - macro_f1: 0.3759 - val_loss: 0.1106 - val_accuracy: 0.5834 - val_macro_f1: 0.3080 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0437 - accuracy: 0.6301 - macro_f1: 0.3890 - val_loss: 0.1075 - val_accuracy: 0.5993 - val_macro_f1: 0.3136 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0422 - accuracy: 0.6321 - macro_f1: 0.4006 - val_loss: 0.1071 - val_accuracy: 0.5997 - val_macro_f1: 0.3140 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0407 - accuracy: 0.6405 - macro_f1: 0.4143 - val_loss: 0.1136 - val_accuracy: 0.5788 - val_macro_f1: 0.3080 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0388 - accuracy: 0.6400 - macro_f1: 0.4205 - val_loss: 0.1132 - val_accuracy: 0.5904 - val_macro_f1: 0.3170 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0366 - accuracy: 0.6521 - macro_f1: 0.4351 - val_loss: 0.1106 - val_accuracy: 0.5979 - val_macro_f1: 0.3259 - lr: 0.0010\n",
      "Epoch 14/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0347 - accuracy: 0.6618 - macro_f1: 0.4429 - val_loss: 0.1119 - val_accuracy: 0.5857 - val_macro_f1: 0.3208 - lr: 0.0010\n",
      "Epoch 15/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0375 - accuracy: 0.6441 - macro_f1: 0.4344 - val_loss: 0.1145 - val_accuracy: 0.5764 - val_macro_f1: 0.3275 - lr: 0.0010\n",
      "Epoch 16/25\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0305 - accuracy: 0.6724 - macro_f1: 0.4675 - val_loss: 0.1070 - val_accuracy: 0.6039 - val_macro_f1: 0.3372 - lr: 1.0000e-04\n",
      "Epoch 17/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0281 - accuracy: 0.6902 - macro_f1: 0.4808 - val_loss: 0.1072 - val_accuracy: 0.6067 - val_macro_f1: 0.3377 - lr: 1.0000e-04\n",
      "Epoch 18/25\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0271 - accuracy: 0.6906 - macro_f1: 0.4890 - val_loss: 0.1083 - val_accuracy: 0.6137 - val_macro_f1: 0.3336 - lr: 1.0000e-04\n",
      "Epoch 19/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0260 - accuracy: 0.6915 - macro_f1: 0.4990 - val_loss: 0.1088 - val_accuracy: 0.6142 - val_macro_f1: 0.3381 - lr: 1.0000e-04\n",
      "Epoch 20/25\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0252 - accuracy: 0.6976 - macro_f1: 0.5030 - val_loss: 0.1077 - val_accuracy: 0.6263 - val_macro_f1: 0.3339 - lr: 1.0000e-04\n",
      "Epoch 21/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0244 - accuracy: 0.7053 - macro_f1: 0.5103 - val_loss: 0.1085 - val_accuracy: 0.6184 - val_macro_f1: 0.3324 - lr: 1.0000e-05\n",
      "Epoch 22/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0242 - accuracy: 0.7073 - macro_f1: 0.5095 - val_loss: 0.1087 - val_accuracy: 0.6202 - val_macro_f1: 0.3369 - lr: 1.0000e-05\n",
      "Epoch 23/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0242 - accuracy: 0.7025 - macro_f1: 0.5127 - val_loss: 0.1089 - val_accuracy: 0.6165 - val_macro_f1: 0.3361 - lr: 1.0000e-05\n",
      "Epoch 24/25\n",
      "267/267 [==============================] - 2s 6ms/step - loss: 0.0241 - accuracy: 0.7028 - macro_f1: 0.5110 - val_loss: 0.1089 - val_accuracy: 0.6202 - val_macro_f1: 0.3341 - lr: 1.0000e-05\n",
      "Epoch 25/25\n",
      "267/267 [==============================] - 2s 7ms/step - loss: 0.0240 - accuracy: 0.7008 - macro_f1: 0.5120 - val_loss: 0.1089 - val_accuracy: 0.6193 - val_macro_f1: 0.3336 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x73db10c291b0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_ewc = 1000\n",
    "cnn_sub_model = modify_model_for_subdiagnostic(cnn_super_model, num_classes_sub)\n",
    "exclude_params_cnn = [id(w) for w in cnn_sub_model.layers[-1].trainable_weights]\n",
    "ewc_cnn = EWC(cnn_super_model, X_train, y_train_super, exclude_params=exclude_params_cnn)\n",
    "\n",
    "def ewc_loss_cnn(y_true, y_pred):\n",
    "    task_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    ewc_penalty = ewc_cnn.penalty(cnn_sub_model)\n",
    "    total_loss = task_loss + (lambda_ewc / 2) * ewc_penalty\n",
    "    return total_loss\n",
    "\n",
    "cnn_sub_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=ewc_loss_cnn,\n",
    "    metrics=[macro_f1]\n",
    ")\n",
    "\n",
    "train_model(cnn_sub_model, X_train, y_train_sub, X_val, y_val_sub, class_weight_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "267/267 [==============================] - 14s 21ms/step - loss: 0.0919 - accuracy: 0.5074 - macro_f1: 0.1920 - val_loss: 12.8419 - val_accuracy: 0.0140 - val_macro_f1: 0.0163 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0824 - accuracy: 0.5193 - macro_f1: 0.2222 - val_loss: 0.1560 - val_accuracy: 0.3607 - val_macro_f1: 0.1969 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0730 - accuracy: 0.5496 - macro_f1: 0.2536 - val_loss: 0.1488 - val_accuracy: 0.4087 - val_macro_f1: 0.1990 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0707 - accuracy: 0.5542 - macro_f1: 0.2728 - val_loss: 0.1373 - val_accuracy: 0.4637 - val_macro_f1: 0.2351 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0618 - accuracy: 0.5713 - macro_f1: 0.3038 - val_loss: 0.1622 - val_accuracy: 0.3672 - val_macro_f1: 0.2348 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0581 - accuracy: 0.5963 - macro_f1: 0.3251 - val_loss: 0.1267 - val_accuracy: 0.5308 - val_macro_f1: 0.2486 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0563 - accuracy: 0.5941 - macro_f1: 0.3280 - val_loss: 0.1272 - val_accuracy: 0.5061 - val_macro_f1: 0.2849 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0565 - accuracy: 0.5915 - macro_f1: 0.3335 - val_loss: 0.1269 - val_accuracy: 0.5876 - val_macro_f1: 0.2739 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0507 - accuracy: 0.6038 - macro_f1: 0.3514 - val_loss: 0.1114 - val_accuracy: 0.5937 - val_macro_f1: 0.3047 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0459 - accuracy: 0.6213 - macro_f1: 0.3762 - val_loss: 0.1106 - val_accuracy: 0.5890 - val_macro_f1: 0.3120 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0439 - accuracy: 0.6341 - macro_f1: 0.3854 - val_loss: 0.1095 - val_accuracy: 0.5890 - val_macro_f1: 0.3107 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0549 - accuracy: 0.5649 - macro_f1: 0.3393 - val_loss: 0.1952 - val_accuracy: 0.5014 - val_macro_f1: 0.2611 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0508 - accuracy: 0.5978 - macro_f1: 0.3553 - val_loss: 0.1241 - val_accuracy: 0.5377 - val_macro_f1: 0.3028 - lr: 0.0010\n",
      "Epoch 14/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0460 - accuracy: 0.6138 - macro_f1: 0.3782 - val_loss: 0.1140 - val_accuracy: 0.5764 - val_macro_f1: 0.3141 - lr: 0.0010\n",
      "Epoch 15/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0385 - accuracy: 0.6373 - macro_f1: 0.4166 - val_loss: 0.1081 - val_accuracy: 0.6156 - val_macro_f1: 0.3153 - lr: 0.0010\n",
      "Epoch 16/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0448 - accuracy: 0.6178 - macro_f1: 0.3894 - val_loss: 0.9500 - val_accuracy: 0.1547 - val_macro_f1: 0.1817 - lr: 0.0010\n",
      "Epoch 17/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0582 - accuracy: 0.5654 - macro_f1: 0.3246 - val_loss: 0.1394 - val_accuracy: 0.4464 - val_macro_f1: 0.2891 - lr: 0.0010\n",
      "Epoch 18/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0450 - accuracy: 0.6211 - macro_f1: 0.3844 - val_loss: 0.1157 - val_accuracy: 0.6021 - val_macro_f1: 0.3218 - lr: 0.0010\n",
      "Epoch 19/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0373 - accuracy: 0.6417 - macro_f1: 0.4245 - val_loss: 0.1121 - val_accuracy: 0.6058 - val_macro_f1: 0.3218 - lr: 0.0010\n",
      "Epoch 20/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0335 - accuracy: 0.6500 - macro_f1: 0.4514 - val_loss: 0.1256 - val_accuracy: 0.6058 - val_macro_f1: 0.3091 - lr: 0.0010\n",
      "Epoch 21/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0275 - accuracy: 0.6796 - macro_f1: 0.4908 - val_loss: 0.1099 - val_accuracy: 0.6188 - val_macro_f1: 0.3356 - lr: 1.0000e-04\n",
      "Epoch 22/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0253 - accuracy: 0.6836 - macro_f1: 0.5009 - val_loss: 0.1144 - val_accuracy: 0.6142 - val_macro_f1: 0.3386 - lr: 1.0000e-04\n",
      "Epoch 23/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0241 - accuracy: 0.6929 - macro_f1: 0.5146 - val_loss: 0.1171 - val_accuracy: 0.5951 - val_macro_f1: 0.3387 - lr: 1.0000e-04\n",
      "Epoch 24/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0230 - accuracy: 0.6970 - macro_f1: 0.5181 - val_loss: 0.1219 - val_accuracy: 0.5932 - val_macro_f1: 0.3416 - lr: 1.0000e-04\n",
      "Epoch 25/25\n",
      "267/267 [==============================] - 5s 21ms/step - loss: 0.0218 - accuracy: 0.7000 - macro_f1: 0.5313 - val_loss: 0.1241 - val_accuracy: 0.6174 - val_macro_f1: 0.3400 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x73db0577ace0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_sub_model = modify_model_for_subdiagnostic(resnet_super_model, num_classes_sub)\n",
    "exclude_params_resnet = [id(w) for w in resnet_sub_model.layers[-1].trainable_weights]\n",
    "ewc_resnet = EWC(resnet_super_model, X_train, y_train_super, exclude_params=exclude_params_resnet)\n",
    "def ewc_loss_resnet(y_true, y_pred):\n",
    "    task_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    ewc_penalty = ewc_resnet.penalty(resnet_sub_model)\n",
    "    total_loss = task_loss + (lambda_ewc / 2) * ewc_penalty\n",
    "    return total_loss\n",
    "\n",
    "resnet_sub_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=ewc_loss_resnet,\n",
    "    metrics=[macro_f1]\n",
    ")\n",
    "\n",
    "train_model(resnet_sub_model, X_train, y_train_sub, X_val, y_val_sub, class_weight_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "267/267 [==============================] - 13s 21ms/step - loss: 0.0994 - accuracy: 0.4673 - macro_f1: 0.1712 - val_loss: 0.1509 - val_accuracy: 0.4897 - val_macro_f1: 0.1730 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0730 - accuracy: 0.5453 - macro_f1: 0.2432 - val_loss: 0.1399 - val_accuracy: 0.4902 - val_macro_f1: 0.2325 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "267/267 [==============================] - 5s 18ms/step - loss: 0.0555 - accuracy: 0.5749 - macro_f1: 0.3039 - val_loss: 0.1271 - val_accuracy: 0.5792 - val_macro_f1: 0.2226 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0454 - accuracy: 0.6127 - macro_f1: 0.3597 - val_loss: 0.1279 - val_accuracy: 0.5760 - val_macro_f1: 0.2620 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0405 - accuracy: 0.6345 - macro_f1: 0.3964 - val_loss: 0.1404 - val_accuracy: 0.5555 - val_macro_f1: 0.2264 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0368 - accuracy: 0.6366 - macro_f1: 0.4113 - val_loss: 0.1371 - val_accuracy: 0.5867 - val_macro_f1: 0.2595 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0298 - accuracy: 0.6699 - macro_f1: 0.4641 - val_loss: 0.1437 - val_accuracy: 0.5573 - val_macro_f1: 0.2665 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0290 - accuracy: 0.6723 - macro_f1: 0.4748 - val_loss: 0.1467 - val_accuracy: 0.5750 - val_macro_f1: 0.2508 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0210 - accuracy: 0.7180 - macro_f1: 0.5274 - val_loss: 0.1478 - val_accuracy: 0.5862 - val_macro_f1: 0.2704 - lr: 1.0000e-04\n",
      "Epoch 10/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0183 - accuracy: 0.7314 - macro_f1: 0.5476 - val_loss: 0.1504 - val_accuracy: 0.5904 - val_macro_f1: 0.2722 - lr: 1.0000e-04\n",
      "Epoch 11/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0170 - accuracy: 0.7372 - macro_f1: 0.5536 - val_loss: 0.1561 - val_accuracy: 0.5708 - val_macro_f1: 0.2755 - lr: 1.0000e-04\n",
      "Epoch 12/25\n",
      "267/267 [==============================] - 5s 20ms/step - loss: 0.0162 - accuracy: 0.7423 - macro_f1: 0.5660 - val_loss: 0.1549 - val_accuracy: 0.5834 - val_macro_f1: 0.2692 - lr: 1.0000e-04\n",
      "Epoch 13/25\n",
      "267/267 [==============================] - 5s 19ms/step - loss: 0.0154 - accuracy: 0.7466 - macro_f1: 0.5741 - val_loss: 0.1562 - val_accuracy: 0.5862 - val_macro_f1: 0.2720 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x73db0625ebc0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_sub_model = modify_model_for_subdiagnostic(vit_super_model, num_classes_sub)\n",
    "exclude_params_vit = [id(w) for w in vit_sub_model.layers[-1].trainable_weights]\n",
    "ewc_vit = EWC(vit_super_model, X_train, y_train_super, exclude_params=exclude_params_vit)\n",
    "\n",
    "def ewc_loss_vit(y_true, y_pred):\n",
    "    task_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    ewc_penalty = ewc_vit.penalty(vit_sub_model)\n",
    "    total_loss = task_loss + (lambda_ewc / 2) * ewc_penalty\n",
    "    return total_loss\n",
    "\n",
    "vit_sub_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=ewc_loss_vit,\n",
    "    metrics=[macro_f1]\n",
    ")\n",
    "\n",
    "train_model(vit_sub_model, X_train, y_train_sub, X_val, y_val_sub, class_weight_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining and Training on SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SI:\n",
    "    def __init__(self, prev_model, damping_factor=0.1, exclude_params=[]):\n",
    "        self.prev_params = {}\n",
    "        self.omega = {}\n",
    "        self.damping_factor = damping_factor\n",
    "        self.exclude_params = exclude_params\n",
    "        \n",
    "        for var in prev_model.trainable_variables:\n",
    "            if var.name not in self.exclude_params:\n",
    "                self.prev_params[var.name] = var.numpy()\n",
    "                self.omega[var.name] = np.zeros_like(var.numpy())\n",
    "\n",
    "    def accumulate_importance(self, model, grads):\n",
    "        for var, grad in zip(model.trainable_variables, grads):\n",
    "            if grad is not None and var.name in self.omega:\n",
    "                delta_theta = var.numpy() - self.prev_params[var.name]\n",
    "                self.omega[var.name] += grad.numpy() * delta_theta\n",
    "\n",
    "    def update_omega(self, model):\n",
    "        for var in model.trainable_variables:\n",
    "            if var.name in self.omega:\n",
    "                delta_theta = var.numpy() - self.prev_params[var.name]\n",
    "                denom = np.square(delta_theta) + self.damping_factor\n",
    "                # Avoid division by zero\n",
    "                denom = np.where(denom == 0, self.damping_factor, denom)\n",
    "                self.omega[var.name] = self.omega[var.name] / denom\n",
    "\n",
    "    def penalty(self, model):\n",
    "        loss = 0\n",
    "        for var in model.trainable_variables:\n",
    "            if var.name in self.omega:\n",
    "                omega = tf.convert_to_tensor(self.omega[var.name], dtype=var.dtype)\n",
    "                prev_param = tf.convert_to_tensor(self.prev_params[var.name], dtype=var.dtype)\n",
    "                loss += tf.reduce_sum(omega * tf.square(var - prev_param))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_sub = y_train_sub.shape[1]\n",
    "cnn_sub_model = modify_model_for_subdiagnostic(cnn_super_model, num_classes_sub)\n",
    "\n",
    "exclude_params_cnn = [w.name for w in cnn_sub_model.layers[-1].trainable_weights]\n",
    "si_cnn = SI(cnn_sub_model, exclude_params=exclude_params_cnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CNN Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/266 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x73db04ef01f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x73db04ef01f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 33.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Time: 7.86s, Loss: 0.1020, Macro F1: 0.3096, Val Loss: 0.0982, Val Macro F1: 0.3241\n",
      "\n",
      "CNN Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25, Time: 7.16s, Loss: nan, Macro F1: 0.1675, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 3/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25, Time: 7.03s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 4/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 38.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25, Time: 7.00s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 5/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25, Time: 7.14s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 6/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25, Time: 7.04s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 7/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25, Time: 7.03s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 8/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25, Time: 7.07s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 9/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25, Time: 7.01s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 10/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25, Time: 7.07s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 11/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25, Time: 7.11s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 12/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25, Time: 7.09s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 13/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25, Time: 7.03s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 14/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25, Time: 7.05s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 15/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25, Time: 7.11s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 16/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25, Time: 7.07s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 17/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25, Time: 7.08s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 18/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25, Time: 7.14s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 19/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25, Time: 7.10s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 20/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25, Time: 7.07s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 21/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25, Time: 7.08s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 22/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25, Time: 7.13s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 23/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25, Time: 7.08s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 24/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25, Time: 7.10s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "CNN Epoch 25/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:07<00:00, 37.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Time: 7.09s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "lambda_si = 1.0\n",
    "epochs = 25\n",
    "batch_size = 64\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_macro_f1 = tf.keras.metrics.Mean(name='train_macro_f1')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "val_macro_f1 = tf.keras.metrics.Mean(name='val_macro_f1')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(f'\\nCNN Epoch {epoch+1}/{epochs}')\n",
    "    train_macro_f1.reset_state()\n",
    "    train_loss.reset_state()\n",
    "    \n",
    "    for step in tqdm(range(len(X_train) // batch_size)):\n",
    "        X_batch = X_train[step*batch_size:(step+1)*batch_size]\n",
    "        y_batch = y_train_sub[step*batch_size:(step+1)*batch_size]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = cnn_sub_model(X_batch, training=True)\n",
    "            task_loss = tf.keras.losses.binary_crossentropy(y_batch, preds)\n",
    "            si_penalty = si_cnn.penalty(cnn_sub_model)\n",
    "            total_loss = tf.reduce_mean(task_loss + lambda_si * si_penalty)\n",
    "        \n",
    "        grads = tape.gradient(total_loss, cnn_sub_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, cnn_sub_model.trainable_variables))\n",
    "        si_cnn.accumulate_importance(cnn_sub_model, grads)\n",
    "        \n",
    "        batch_macro_f1 = macro_f1(y_batch, preds)\n",
    "        train_macro_f1.update_state(batch_macro_f1)\n",
    "        train_loss.update_state(total_loss)\n",
    "        \n",
    "    si_cnn.update_omega(cnn_sub_model)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    val_macro_f1.reset_state()\n",
    "    val_loss.reset_state()\n",
    "    for step in range(len(X_val) // batch_size):\n",
    "        X_batch = X_val[step*batch_size:(step+1)*batch_size]\n",
    "        y_batch = y_val_sub[step*batch_size:(step+1)*batch_size]\n",
    "        preds = cnn_sub_model(X_batch, training=False)\n",
    "        task_loss = tf.keras.losses.binary_crossentropy(y_batch, preds)\n",
    "        total_loss = tf.reduce_mean(task_loss)\n",
    "\n",
    "        batch_macro_f1 = macro_f1(y_batch, preds)\n",
    "        val_macro_f1.update_state(batch_macro_f1)\n",
    "        val_loss.update_state(total_loss)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, '\n",
    "          f'Time: {epoch_time:.2f}s, '\n",
    "          f'Loss: {train_loss.result():.4f}, '\n",
    "          f'Macro F1: {train_macro_f1.result():.4f}, '\n",
    "          f'Val Loss: {val_loss.result():.4f}, '\n",
    "          f'Val Macro F1: {val_macro_f1.result():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_sub_model = modify_model_for_subdiagnostic(resnet_super_model, num_classes_sub)\n",
    "exclude_params_resnet = [w.name for w in resnet_sub_model.layers[-1].trainable_weights]\n",
    "si_resnet = SI(resnet_sub_model, exclude_params=exclude_params_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ResNet Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:49<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Time: 49.55s, Loss: 0.0876, Macro F1: 0.2896, Val Loss: 0.1146, Val Macro F1: 0.2826\n",
      "\n",
      "ResNet Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:45<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25, Time: 45.33s, Loss: nan, Macro F1: 0.1216, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 3/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:45<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25, Time: 45.44s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 4/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:44<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25, Time: 44.84s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 5/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:45<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25, Time: 45.43s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 6/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:45<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25, Time: 45.19s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 7/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:44<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25, Time: 44.85s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 8/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:45<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25, Time: 45.39s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 9/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:45<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25, Time: 45.44s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 10/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:45<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25, Time: 45.05s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 11/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:45<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25, Time: 45.54s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 12/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:45<00:00,  5.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25, Time: 45.51s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 13/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:44<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25, Time: 44.84s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 14/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:45<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25, Time: 45.25s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 15/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:45<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25, Time: 45.24s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 16/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:45<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25, Time: 45.03s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 17/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:45<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25, Time: 45.31s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 18/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:45<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25, Time: 45.39s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 19/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:45<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25, Time: 45.06s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 20/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:45<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25, Time: 45.72s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 21/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:44<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25, Time: 44.94s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 22/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:45<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25, Time: 45.14s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 23/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:45<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25, Time: 45.27s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 24/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:44<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25, Time: 44.76s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ResNet Epoch 25/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:45<00:00,  5.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Time: 45.47s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "lambda_si = 1.0  \n",
    "epochs = 25\n",
    "batch_size = 64\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_macro_f1 = tf.keras.metrics.Mean(name='train_macro_f1')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "val_macro_f1 = tf.keras.metrics.Mean(name='val_macro_f1')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(f'\\nResNet Epoch {epoch+1}/{epochs}')\n",
    "    train_macro_f1.reset_state()\n",
    "    train_loss.reset_state()\n",
    "    \n",
    "    for step in tqdm(range(len(X_train) // batch_size)):\n",
    "        X_batch = X_train[step*batch_size:(step+1)*batch_size]\n",
    "        y_batch = y_train_sub[step*batch_size:(step+1)*batch_size]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = resnet_sub_model(X_batch, training=True)\n",
    "            task_loss = tf.keras.losses.binary_crossentropy(y_batch, preds)\n",
    "            si_penalty = si_resnet.penalty(resnet_sub_model)\n",
    "            total_loss = tf.reduce_mean(task_loss + lambda_si * si_penalty)\n",
    "        \n",
    "        grads = tape.gradient(total_loss, resnet_sub_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, resnet_sub_model.trainable_variables))\n",
    "        si_resnet.accumulate_importance(resnet_sub_model, grads)\n",
    "    \n",
    "        batch_macro_f1 = macro_f1(y_batch, preds)\n",
    "        train_macro_f1.update_state(batch_macro_f1)\n",
    "        train_loss.update_state(total_loss)\n",
    "    \n",
    "    si_resnet.update_omega(resnet_sub_model)\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    val_macro_f1.reset_state()\n",
    "    val_loss.reset_state()\n",
    "    for step in range(len(X_val) // batch_size):\n",
    "        X_batch = X_val[step*batch_size:(step+1)*batch_size]\n",
    "        y_batch = y_val_sub[step*batch_size:(step+1)*batch_size]\n",
    "        preds = resnet_sub_model(X_batch, training=False)\n",
    "        task_loss = tf.keras.losses.binary_crossentropy(y_batch, preds)\n",
    "        total_loss = tf.reduce_mean(task_loss)\n",
    "\n",
    "        batch_macro_f1 = macro_f1(y_batch, preds)\n",
    "        val_macro_f1.update_state(batch_macro_f1)\n",
    "        val_loss.update_state(total_loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, '\n",
    "          f'Time: {epoch_time:.2f}s, '\n",
    "          f'Loss: {train_loss.result():.4f}, '\n",
    "          f'Macro F1: {train_macro_f1.result():.4f}, '\n",
    "          f'Val Loss: {val_loss.result():.4f}, '\n",
    "          f'Val Macro F1: {val_macro_f1.result():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_sub_model = modify_model_for_subdiagnostic(vit_super_model, num_classes_sub)\n",
    "exclude_params_vit = [w.name for w in vit_sub_model.layers[-1].trainable_weights]\n",
    "si_vit = SI(vit_sub_model, exclude_params=exclude_params_vit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ViT Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:40<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Time: 40.37s, Loss: 0.1119, Macro F1: 0.2410, Val Loss: 0.1109, Val Macro F1: 0.2454\n",
      "\n",
      "ViT Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25, Time: 35.15s, Loss: nan, Macro F1: 0.1772, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 3/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25, Time: 35.19s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 4/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25, Time: 35.33s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 5/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25, Time: 35.76s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 6/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25, Time: 35.24s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 7/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25, Time: 35.59s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 8/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25, Time: 35.23s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 9/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:34<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25, Time: 34.96s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 10/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25, Time: 35.19s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 11/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25, Time: 35.68s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 12/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25, Time: 35.46s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 13/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25, Time: 35.33s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 14/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25, Time: 35.65s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 15/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25, Time: 35.16s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 16/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25, Time: 35.31s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 17/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25, Time: 35.66s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 18/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25, Time: 35.08s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 19/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25, Time: 35.02s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 20/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25, Time: 35.69s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 21/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25, Time: 35.13s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 22/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25, Time: 35.24s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 23/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25, Time: 35.92s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 24/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25, Time: 35.40s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n",
      "\n",
      "ViT Epoch 25/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [00:35<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Time: 35.30s, Loss: nan, Macro F1: 0.0000, Val Loss: nan, Val Macro F1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "lambda_si = 1.0\n",
    "epochs = 25\n",
    "batch_size = 64\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_macro_f1 = tf.keras.metrics.Mean(name='train_macro_f1')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "val_macro_f1 = tf.keras.metrics.Mean(name='val_macro_f1')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(f'\\nViT Epoch {epoch+1}/{epochs}')\n",
    "    \n",
    "    train_macro_f1.reset_state()\n",
    "    train_loss.reset_state()\n",
    "\n",
    "    for step in tqdm(range(len(X_train) // batch_size)):\n",
    "        X_batch = X_train[step*batch_size:(step+1)*batch_size]\n",
    "        y_batch = y_train_sub[step*batch_size:(step+1)*batch_size]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = vit_sub_model(X_batch, training=True)\n",
    "            task_loss = tf.keras.losses.binary_crossentropy(y_batch, preds)\n",
    "            si_penalty = si_vit.penalty(vit_sub_model)\n",
    "            total_loss = tf.reduce_mean(task_loss + lambda_si * si_penalty)\n",
    "        \n",
    "        grads = tape.gradient(total_loss, vit_sub_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, vit_sub_model.trainable_variables))\n",
    "        \n",
    "        si_vit.accumulate_importance(vit_sub_model, grads)\n",
    "        batch_macro_f1 = macro_f1(y_batch, preds)\n",
    "        train_macro_f1.update_state(batch_macro_f1)\n",
    "        train_loss.update_state(total_loss)\n",
    "        \n",
    "    si_vit.update_omega(vit_sub_model)\n",
    "    epoch_time = time.time() - start_time\n",
    "    val_macro_f1.reset_state()\n",
    "    val_loss.reset_state()\n",
    "    for step in range(len(X_val) // batch_size):\n",
    "        X_batch = X_val[step*batch_size:(step+1)*batch_size]\n",
    "        y_batch = y_val_sub[step*batch_size:(step+1)*batch_size]\n",
    "        preds = vit_sub_model(X_batch, training=False)\n",
    "        task_loss = tf.keras.losses.binary_crossentropy(y_batch, preds)\n",
    "        total_loss = tf.reduce_mean(task_loss)\n",
    "        \n",
    "        batch_macro_f1 = macro_f1(y_batch, preds)\n",
    "        val_macro_f1.update_state(batch_macro_f1)\n",
    "        val_loss.update_state(total_loss)\n",
    "        \n",
    "    print(f'Epoch {epoch+1}/{epochs}, '\n",
    "          f'Time: {epoch_time:.2f}s, '\n",
    "          f'Loss: {train_loss.result():.4f}, '\n",
    "          f'Macro F1: {train_macro_f1.result():.4f}, '\n",
    "          f'Val Loss: {val_loss.result():.4f}, '\n",
    "          f'Val Macro F1: {val_macro_f1.result():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling and Publishing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN EWC Subdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 716us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.00      0.00      0.00       306\n",
      "       CLBBB       0.00      0.00      0.00        54\n",
      "       CRBBB       0.00      0.00      0.00        54\n",
      "       ILBBB       0.00      0.00      0.00         8\n",
      "         IMI       0.00      0.00      0.00       327\n",
      "       IRBBB       0.00      0.00      0.00       112\n",
      "        ISCA       0.00      0.00      0.00        93\n",
      "        ISCI       0.00      0.00      0.00        40\n",
      "        ISC_       0.00      0.00      0.00       128\n",
      "        IVCD       0.00      0.00      0.00        79\n",
      "   LAFB/LPFB       0.00      0.00      0.00       179\n",
      "     LAO/LAE       0.00      0.00      0.00        42\n",
      "         LMI       0.00      0.00      0.00        20\n",
      "         LVH       0.00      0.00      0.00       214\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        NST_       0.00      0.00      0.00        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.00      0.00      0.00        10\n",
      "         RVH       0.00      0.00      0.00        12\n",
      "       SEHYP       0.00      0.00      0.00         2\n",
      "        STTC       0.00      0.00      0.00       222\n",
      "         WPW       0.00      0.00      0.00         8\n",
      "        _AVB       0.00      0.00      0.00        82\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      3034\n",
      "   macro avg       0.00      0.00      0.00      3034\n",
      "weighted avg       0.00      0.00      0.00      3034\n",
      " samples avg       0.00      0.00      0.00      3034\n",
      "\n",
      "CNN EWC Superdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.00      0.00      0.00       496\n",
      "         HYP       0.00      0.00      0.00       262\n",
      "          MI       0.00      0.00      0.00       550\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        STTC       0.00      0.00      0.00       521\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      2792\n",
      "   macro avg       0.00      0.00      0.00      2792\n",
      "weighted avg       0.00      0.00      0.00      2792\n",
      " samples avg       0.00      0.00      0.00      2792\n",
      "\n",
      "ResNet EWC Subdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.00      0.00      0.00       306\n",
      "       CLBBB       0.00      0.00      0.00        54\n",
      "       CRBBB       0.00      0.00      0.00        54\n",
      "       ILBBB       0.00      0.00      0.00         8\n",
      "         IMI       0.00      0.00      0.00       327\n",
      "       IRBBB       0.00      0.00      0.00       112\n",
      "        ISCA       0.00      0.00      0.00        93\n",
      "        ISCI       0.00      0.00      0.00        40\n",
      "        ISC_       0.00      0.00      0.00       128\n",
      "        IVCD       0.00      0.00      0.00        79\n",
      "   LAFB/LPFB       0.00      0.00      0.00       179\n",
      "     LAO/LAE       0.00      0.00      0.00        42\n",
      "         LMI       0.00      0.00      0.00        20\n",
      "         LVH       0.00      0.00      0.00       214\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        NST_       0.00      0.00      0.00        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.00      0.00      0.00        10\n",
      "         RVH       0.00      0.00      0.00        12\n",
      "       SEHYP       0.00      0.00      0.00         2\n",
      "        STTC       0.00      0.00      0.00       222\n",
      "         WPW       0.00      0.00      0.00         8\n",
      "        _AVB       0.00      0.00      0.00        82\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      3034\n",
      "   macro avg       0.00      0.00      0.00      3034\n",
      "weighted avg       0.00      0.00      0.00      3034\n",
      " samples avg       0.00      0.00      0.00      3034\n",
      "\n",
      "ResNet EWC Superdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.00      0.00      0.00       496\n",
      "         HYP       0.00      0.00      0.00       262\n",
      "          MI       0.00      0.00      0.00       550\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        STTC       0.00      0.00      0.00       521\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      2792\n",
      "   macro avg       0.00      0.00      0.00      2792\n",
      "weighted avg       0.00      0.00      0.00      2792\n",
      " samples avg       0.00      0.00      0.00      2792\n",
      "\n",
      "ViT EWC Subdiagnostic Classification Report:\n",
      "68/68 [==============================] - 1s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.00      0.00      0.00       306\n",
      "       CLBBB       0.00      0.00      0.00        54\n",
      "       CRBBB       0.00      0.00      0.00        54\n",
      "       ILBBB       0.00      0.00      0.00         8\n",
      "         IMI       0.00      0.00      0.00       327\n",
      "       IRBBB       0.00      0.00      0.00       112\n",
      "        ISCA       0.00      0.00      0.00        93\n",
      "        ISCI       0.00      0.00      0.00        40\n",
      "        ISC_       0.00      0.00      0.00       128\n",
      "        IVCD       0.00      0.00      0.00        79\n",
      "   LAFB/LPFB       0.00      0.00      0.00       179\n",
      "     LAO/LAE       0.00      0.00      0.00        42\n",
      "         LMI       0.00      0.00      0.00        20\n",
      "         LVH       0.00      0.00      0.00       214\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        NST_       0.00      0.00      0.00        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.00      0.00      0.00        10\n",
      "         RVH       0.00      0.00      0.00        12\n",
      "       SEHYP       0.00      0.00      0.00         2\n",
      "        STTC       0.00      0.00      0.00       222\n",
      "         WPW       0.00      0.00      0.00         8\n",
      "        _AVB       0.00      0.00      0.00        82\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      3034\n",
      "   macro avg       0.00      0.00      0.00      3034\n",
      "weighted avg       0.00      0.00      0.00      3034\n",
      " samples avg       0.00      0.00      0.00      3034\n",
      "\n",
      "ViT EWC Superdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.00      0.00      0.00       496\n",
      "         HYP       0.00      0.00      0.00       262\n",
      "          MI       0.00      0.00      0.00       550\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        STTC       0.00      0.00      0.00       521\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      2792\n",
      "   macro avg       0.00      0.00      0.00      2792\n",
      "weighted avg       0.00      0.00      0.00      2792\n",
      " samples avg       0.00      0.00      0.00      2792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN EWC Subdiagnostic Classification Report:\")\n",
    "cnn_ewc_sub_report = evaluate_model(cnn_sub_model, X_test, y_test_sub, classes_sub)\n",
    "\n",
    "print(\"CNN EWC Superdiagnostic Classification Report:\")\n",
    "cnn_ewc_super_report = evaluate_model(cnn_super_model, X_test, y_test_super, classes_super)\n",
    "\n",
    "print(\"ResNet EWC Subdiagnostic Classification Report:\")\n",
    "resnet_ewc_sub_report = evaluate_model(resnet_sub_model, X_test, y_test_sub, classes_sub)\n",
    "\n",
    "print(\"ResNet EWC Superdiagnostic Classification Report:\")\n",
    "resnet_ewc_super_report = evaluate_model(resnet_super_model, X_test, y_test_super, classes_super)\n",
    "\n",
    "print(\"ViT EWC Subdiagnostic Classification Report:\")\n",
    "vit_ewc_sub_report = evaluate_model(vit_sub_model, X_test, y_test_sub, classes_sub)\n",
    "\n",
    "print(\"ViT EWC Superdiagnostic Classification Report:\")\n",
    "vit_ewc_super_report = evaluate_model(vit_super_model, X_test, y_test_super, classes_super)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN SI Subdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 886us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.00      0.00      0.00       306\n",
      "       CLBBB       0.00      0.00      0.00        54\n",
      "       CRBBB       0.00      0.00      0.00        54\n",
      "       ILBBB       0.00      0.00      0.00         8\n",
      "         IMI       0.00      0.00      0.00       327\n",
      "       IRBBB       0.00      0.00      0.00       112\n",
      "        ISCA       0.00      0.00      0.00        93\n",
      "        ISCI       0.00      0.00      0.00        40\n",
      "        ISC_       0.00      0.00      0.00       128\n",
      "        IVCD       0.00      0.00      0.00        79\n",
      "   LAFB/LPFB       0.00      0.00      0.00       179\n",
      "     LAO/LAE       0.00      0.00      0.00        42\n",
      "         LMI       0.00      0.00      0.00        20\n",
      "         LVH       0.00      0.00      0.00       214\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        NST_       0.00      0.00      0.00        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.00      0.00      0.00        10\n",
      "         RVH       0.00      0.00      0.00        12\n",
      "       SEHYP       0.00      0.00      0.00         2\n",
      "        STTC       0.00      0.00      0.00       222\n",
      "         WPW       0.00      0.00      0.00         8\n",
      "        _AVB       0.00      0.00      0.00        82\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      3034\n",
      "   macro avg       0.00      0.00      0.00      3034\n",
      "weighted avg       0.00      0.00      0.00      3034\n",
      " samples avg       0.00      0.00      0.00      3034\n",
      "\n",
      "CNN SI Superdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 909us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.00      0.00      0.00       496\n",
      "         HYP       0.00      0.00      0.00       262\n",
      "          MI       0.00      0.00      0.00       550\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        STTC       0.00      0.00      0.00       521\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      2792\n",
      "   macro avg       0.00      0.00      0.00      2792\n",
      "weighted avg       0.00      0.00      0.00      2792\n",
      " samples avg       0.00      0.00      0.00      2792\n",
      "\n",
      "ResNet SI Subdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.00      0.00      0.00       306\n",
      "       CLBBB       0.00      0.00      0.00        54\n",
      "       CRBBB       0.00      0.00      0.00        54\n",
      "       ILBBB       0.00      0.00      0.00         8\n",
      "         IMI       0.00      0.00      0.00       327\n",
      "       IRBBB       0.00      0.00      0.00       112\n",
      "        ISCA       0.00      0.00      0.00        93\n",
      "        ISCI       0.00      0.00      0.00        40\n",
      "        ISC_       0.00      0.00      0.00       128\n",
      "        IVCD       0.00      0.00      0.00        79\n",
      "   LAFB/LPFB       0.00      0.00      0.00       179\n",
      "     LAO/LAE       0.00      0.00      0.00        42\n",
      "         LMI       0.00      0.00      0.00        20\n",
      "         LVH       0.00      0.00      0.00       214\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        NST_       0.00      0.00      0.00        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.00      0.00      0.00        10\n",
      "         RVH       0.00      0.00      0.00        12\n",
      "       SEHYP       0.00      0.00      0.00         2\n",
      "        STTC       0.00      0.00      0.00       222\n",
      "         WPW       0.00      0.00      0.00         8\n",
      "        _AVB       0.00      0.00      0.00        82\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      3034\n",
      "   macro avg       0.00      0.00      0.00      3034\n",
      "weighted avg       0.00      0.00      0.00      3034\n",
      " samples avg       0.00      0.00      0.00      3034\n",
      "\n",
      "ResNet SI Superdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.00      0.00      0.00       496\n",
      "         HYP       0.00      0.00      0.00       262\n",
      "          MI       0.00      0.00      0.00       550\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        STTC       0.00      0.00      0.00       521\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      2792\n",
      "   macro avg       0.00      0.00      0.00      2792\n",
      "weighted avg       0.00      0.00      0.00      2792\n",
      " samples avg       0.00      0.00      0.00      2792\n",
      "\n",
      "ViT SI Subdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.00      0.00      0.00       306\n",
      "       CLBBB       0.00      0.00      0.00        54\n",
      "       CRBBB       0.00      0.00      0.00        54\n",
      "       ILBBB       0.00      0.00      0.00         8\n",
      "         IMI       0.00      0.00      0.00       327\n",
      "       IRBBB       0.00      0.00      0.00       112\n",
      "        ISCA       0.00      0.00      0.00        93\n",
      "        ISCI       0.00      0.00      0.00        40\n",
      "        ISC_       0.00      0.00      0.00       128\n",
      "        IVCD       0.00      0.00      0.00        79\n",
      "   LAFB/LPFB       0.00      0.00      0.00       179\n",
      "     LAO/LAE       0.00      0.00      0.00        42\n",
      "         LMI       0.00      0.00      0.00        20\n",
      "         LVH       0.00      0.00      0.00       214\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        NST_       0.00      0.00      0.00        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.00      0.00      0.00        10\n",
      "         RVH       0.00      0.00      0.00        12\n",
      "       SEHYP       0.00      0.00      0.00         2\n",
      "        STTC       0.00      0.00      0.00       222\n",
      "         WPW       0.00      0.00      0.00         8\n",
      "        _AVB       0.00      0.00      0.00        82\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      3034\n",
      "   macro avg       0.00      0.00      0.00      3034\n",
      "weighted avg       0.00      0.00      0.00      3034\n",
      " samples avg       0.00      0.00      0.00      3034\n",
      "\n",
      "ViT SI Superdiagnostic Classification Report:\n",
      "68/68 [==============================] - 0s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.00      0.00      0.00       496\n",
      "         HYP       0.00      0.00      0.00       262\n",
      "          MI       0.00      0.00      0.00       550\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        STTC       0.00      0.00      0.00       521\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      2792\n",
      "   macro avg       0.00      0.00      0.00      2792\n",
      "weighted avg       0.00      0.00      0.00      2792\n",
      " samples avg       0.00      0.00      0.00      2792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN SI Subdiagnostic Classification Report:\")\n",
    "cnn_si_sub_report = evaluate_model(cnn_sub_model, X_test, y_test_sub, classes_sub)\n",
    "\n",
    "print(\"CNN SI Superdiagnostic Classification Report:\")\n",
    "cnn_si_super_report = evaluate_model(cnn_super_model, X_test, y_test_super, classes_super)\n",
    "\n",
    "print(\"ResNet SI Subdiagnostic Classification Report:\")\n",
    "resnet_si_sub_report = evaluate_model(resnet_sub_model, X_test, y_test_sub, classes_sub)\n",
    "\n",
    "print(\"ResNet SI Superdiagnostic Classification Report:\")\n",
    "resnet_si_super_report = evaluate_model(resnet_super_model, X_test, y_test_super, classes_super)\n",
    "\n",
    "print(\"ViT SI Subdiagnostic Classification Report:\")\n",
    "vit_si_sub_report = evaluate_model(vit_sub_model, X_test, y_test_sub, classes_sub)\n",
    "\n",
    "print(\"ViT SI Superdiagnostic Classification Report:\")\n",
    "vit_si_super_report = evaluate_model(vit_super_model, X_test, y_test_super, classes_super)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Classification Performance:\n",
      "     Model                 Task  Macro F1-score\n",
      "0      CNN      Superdiagnostic        0.732153\n",
      "1   ResNet      Superdiagnostic        0.734195\n",
      "2      ViT      Superdiagnostic        0.673153\n",
      "3      CNN        Subdiagnostic        0.419499\n",
      "4   ResNet        Subdiagnostic        0.400487\n",
      "5      ViT        Subdiagnostic        0.279755\n",
      "6      CNN    EWC Subdiagnostic        0.000000\n",
      "7   ResNet    EWC Subdiagnostic        0.000000\n",
      "8      ViT    EWC Subdiagnostic        0.000000\n",
      "9      CNN  EWC Superdiagnostic        0.000000\n",
      "10  ResNet  EWC Superdiagnostic        0.000000\n",
      "11     ViT  EWC Superdiagnostic        0.000000\n",
      "12     CNN     SI Subdiagnostic        0.000000\n",
      "13  ResNet     SI Subdiagnostic        0.000000\n",
      "14     ViT     SI Subdiagnostic        0.000000\n",
      "15     CNN   SI Superdiagnostic        0.000000\n",
      "16  ResNet   SI Superdiagnostic        0.000000\n",
      "17     ViT   SI Superdiagnostic        0.000000\n"
     ]
    }
   ],
   "source": [
    "def get_macro_f1(report_dict):\n",
    "    return report_dict['macro avg']['f1-score']\n",
    "\n",
    "results = {\n",
    "    'Model': [],\n",
    "    'Task': [],\n",
    "    'Macro F1-score': []\n",
    "}\n",
    "\n",
    "results['Model'].extend(['CNN', 'ResNet', 'ViT'])\n",
    "results['Task'].extend(['Superdiagnostic'] * 3)\n",
    "results['Macro F1-score'].extend([\n",
    "    get_macro_f1(cnn_super_report),\n",
    "    get_macro_f1(resnet_super_report),\n",
    "    get_macro_f1(vit_super_report)\n",
    "])\n",
    "\n",
    "results['Model'].extend(['CNN', 'ResNet', 'ViT'])\n",
    "results['Task'].extend(['Subdiagnostic'] * 3)\n",
    "results['Macro F1-score'].extend([\n",
    "    get_macro_f1(cnn_sub_report),\n",
    "    get_macro_f1(resnet_sub_report),\n",
    "    get_macro_f1(vit_sub_report)\n",
    "])\n",
    "\n",
    "results['Model'].extend(['CNN', 'ResNet', 'ViT'])\n",
    "results['Task'].extend(['EWC Subdiagnostic'] * 3)\n",
    "results['Macro F1-score'].extend([\n",
    "    get_macro_f1(cnn_ewc_sub_report),\n",
    "    get_macro_f1(resnet_ewc_sub_report),\n",
    "    get_macro_f1(vit_ewc_sub_report)\n",
    "])\n",
    "\n",
    "results['Model'].extend(['CNN', 'ResNet', 'ViT'])\n",
    "results['Task'].extend(['EWC Superdiagnostic'] * 3)\n",
    "results['Macro F1-score'].extend([\n",
    "    get_macro_f1(cnn_ewc_super_report),\n",
    "    get_macro_f1(resnet_ewc_super_report),\n",
    "    get_macro_f1(vit_ewc_super_report)\n",
    "])\n",
    "\n",
    "results['Model'].extend(['CNN', 'ResNet', 'ViT'])\n",
    "results['Task'].extend(['SI Subdiagnostic'] * 3)\n",
    "results['Macro F1-score'].extend([\n",
    "    get_macro_f1(cnn_si_sub_report),\n",
    "    get_macro_f1(resnet_si_sub_report),\n",
    "    get_macro_f1(vit_si_sub_report)\n",
    "])\n",
    "\n",
    "results['Model'].extend(['CNN', 'ResNet', 'ViT'])\n",
    "results['Task'].extend(['SI Superdiagnostic'] * 3)\n",
    "results['Macro F1-score'].extend([\n",
    "    get_macro_f1(cnn_si_super_report),\n",
    "    get_macro_f1(resnet_si_super_report),\n",
    "    get_macro_f1(vit_si_super_report)\n",
    "])\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of Classification Performance:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Driven code for Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Federated Learning with Continual Learning (EWC & SI)\n",
      "\n",
      "--- Federated Training for CNN ---\n",
      "\n",
      "Communication Round 1/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "\n",
      "Communication Round 2/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "\n",
      "Communication Round 3/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "\n",
      "Communication Round 4/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "\n",
      "Communication Round 5/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "\n",
      "--- Evaluation for CNN after Federated Learning ---\n",
      "\n",
      "Evaluating Global CNN Model on Superdiagnostic Task:\n",
      "68/68 [==============================] - 0s 714us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.92      0.49      0.64       496\n",
      "         HYP       0.91      0.19      0.32       262\n",
      "          MI       0.88      0.45      0.59       550\n",
      "        NORM       0.78      0.94      0.85       963\n",
      "        STTC       0.80      0.65      0.72       521\n",
      "\n",
      "   micro avg       0.82      0.64      0.72      2792\n",
      "   macro avg       0.86      0.55      0.62      2792\n",
      "weighted avg       0.84      0.64      0.69      2792\n",
      " samples avg       0.73      0.68      0.69      2792\n",
      "\n",
      "\n",
      "Evaluating Global CNN Model on Subdiagnostic Task:\n",
      "68/68 [==============================] - 0s 951us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.12      0.64      0.21       306\n",
      "       CLBBB       0.00      0.06      0.00        54\n",
      "       CRBBB       0.06      1.00      0.12        54\n",
      "       ILBBB       0.00      0.50      0.01         8\n",
      "         IMI       0.10      0.49      0.16       327\n",
      "       IRBBB       0.02      0.37      0.05       112\n",
      "        ISCA       0.04      0.74      0.07        93\n",
      "        ISCI       0.08      0.20      0.11        40\n",
      "        ISC_       0.04      0.64      0.08       128\n",
      "        IVCD       0.04      0.62      0.07        79\n",
      "   LAFB/LPFB       0.06      0.64      0.11       179\n",
      "     LAO/LAE       0.06      0.36      0.10        42\n",
      "         LMI       0.02      0.35      0.03        20\n",
      "         LVH       0.10      0.94      0.17       214\n",
      "        NORM       0.13      0.05      0.07       963\n",
      "        NST_       0.07      0.49      0.13        77\n",
      "         PMI       0.00      0.00      0.00         2\n",
      "     RAO/RAE       0.00      1.00      0.01        10\n",
      "         RVH       0.01      0.17      0.01        12\n",
      "       SEHYP       0.00      1.00      0.00         2\n",
      "        STTC       0.05      0.09      0.06       222\n",
      "         WPW       0.01      0.88      0.01         8\n",
      "        _AVB       0.03      0.48      0.05        82\n",
      "\n",
      "   micro avg       0.04      0.38      0.08      3034\n",
      "   macro avg       0.05      0.51      0.07      3034\n",
      "weighted avg       0.09      0.38      0.10      3034\n",
      " samples avg       0.04      0.32      0.07      3034\n",
      "\n",
      "\n",
      "--- Federated Training for ResNet ---\n",
      "\n",
      "Communication Round 1/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "\n",
      "Communication Round 2/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "\n",
      "Communication Round 3/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "\n",
      "Communication Round 4/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "\n",
      "Communication Round 5/5\n",
      " - Client 1/5 local training\n",
      " - Client 2/5 local training\n",
      " - Client 3/5 local training\n",
      " - Client 4/5 local training\n",
      " - Client 5/5 local training\n",
      "\n",
      "--- Evaluation for ResNet after Federated Learning ---\n",
      "\n",
      "Evaluating Global ResNet Model on Superdiagnostic Task:\n",
      "68/68 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.93      0.48      0.64       496\n",
      "         HYP       0.51      0.68      0.58       262\n",
      "          MI       0.72      0.59      0.64       550\n",
      "        NORM       0.80      0.91      0.85       963\n",
      "        STTC       0.83      0.59      0.69       521\n",
      "\n",
      "   micro avg       0.76      0.69      0.72      2792\n",
      "   macro avg       0.76      0.65      0.68      2792\n",
      "weighted avg       0.78      0.69      0.72      2792\n",
      " samples avg       0.73      0.71      0.71      2792\n",
      "\n",
      "\n",
      "Evaluating Global ResNet Model on Subdiagnostic Task:\n",
      "68/68 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMI       0.10      0.25      0.15       306\n",
      "       CLBBB       0.00      0.00      0.00        54\n",
      "       CRBBB       0.04      0.96      0.07        54\n",
      "       ILBBB       0.00      1.00      0.01         8\n",
      "         IMI       0.01      0.02      0.01       327\n",
      "       IRBBB       0.06      0.88      0.12       112\n",
      "        ISCA       0.06      1.00      0.11        93\n",
      "        ISCI       0.00      0.07      0.01        40\n",
      "        ISC_       0.08      0.94      0.15       128\n",
      "        IVCD       0.03      0.11      0.05        79\n",
      "   LAFB/LPFB       0.00      0.00      0.00       179\n",
      "     LAO/LAE       0.03      0.83      0.06        42\n",
      "         LMI       0.00      0.05      0.00        20\n",
      "         LVH       0.03      0.16      0.05       214\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        NST_       0.04      0.99      0.08        77\n",
      "         PMI       0.00      0.50      0.00         2\n",
      "     RAO/RAE       0.00      1.00      0.01        10\n",
      "         RVH       0.01      0.25      0.02        12\n",
      "       SEHYP       0.00      0.00      0.00         2\n",
      "        STTC       0.14      0.95      0.25       222\n",
      "         WPW       0.00      1.00      0.01         8\n",
      "        _AVB       0.00      0.02      0.01        82\n",
      "\n",
      "   micro avg       0.04      0.28      0.06      3034\n",
      "   macro avg       0.03      0.48      0.05      3034\n",
      "weighted avg       0.04      0.28      0.06      3034\n",
      " samples avg       0.03      0.24      0.06      3034\n",
      "\n",
      "\n",
      "--- Federated Training for ViT ---\n",
      "\n",
      "Communication Round 1/5\n",
      " - Client 1/5 local training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_715064/2822895630.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'CNN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ResNet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ViT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mnum_classes_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train_sub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;31m# Federated training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     super_report, sub_report = federated_training(\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mcreate_model_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_model_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_715064/2822895630.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model_type, create_model_fn, input_shape, num_classes_super, num_classes_sub, classes_super, classes_sub)\u001b[0m\n\u001b[1;32m     90\u001b[0m             )\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Initialize EWC for the client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mexclude_params_super\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclient_model_super\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mewc_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEWC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_model_super\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_client_super\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude_params_super\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# --------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;31m# Local Training on Subdiagnostic Task with EWC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_715064/3946864442.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, model, X, y, batch_size, exclude_params)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfisher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_fisher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_715064/3946864442.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, batch_size, exclude_params)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0mparam_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ecg_env/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1108\u001b[0m               output_gradients))\n\u001b[1;32m   1109\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[1;32m   1110\u001b[0m                           for x in output_gradients]\n\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ecg_env/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise ValueError(\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ecg_env/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gradient_tape/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ecg_env/lib/python3.10/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1460\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m   return (array_ops.reshape(\n\u001b[1;32m   1463\u001b[0m       math_ops.reduce_sum(math_ops.realdiv(grad, y), rx), sx),\n\u001b[0;32m-> 1464\u001b[0;31m           array_ops.reshape(\n\u001b[0m\u001b[1;32m   1465\u001b[0m               math_ops.reduce_sum(\n\u001b[1;32m   1466\u001b[0m                   grad * math_ops.realdiv(math_ops.realdiv(-x, y), y), ry), sy))  # pylint: disable=invalid-unary-operand-type\n",
      "\u001b[0;32m~/anaconda3/envs/ecg_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ecg_env/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ecg_env/lib/python3.10/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m   \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m   \"\"\"\n\u001b[0;32m--> 199\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ecg_env/lib/python3.10/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   8539\u001b[0m         _ctx, \"Reshape\", name, tensor, shape)\n\u001b[1;32m   8540\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8541\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8542\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8543\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8544\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8545\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8546\u001b[0m       return reshape_eager_fallback(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# Federated Learning with Continual Learning (EWC & SI)\n",
    "# ----------------------------------------\n",
    "\n",
    "print(\"Starting Federated Learning with Continual Learning (EWC & SI)\")\n",
    "\n",
    "# Number of clients to simulate\n",
    "num_clients = 5\n",
    "\n",
    "# Communication rounds\n",
    "communication_rounds = 5\n",
    "\n",
    "# Local training epochs per client\n",
    "local_epochs = 1\n",
    "\n",
    "# Batch size for local training\n",
    "local_batch_size = 64\n",
    "\n",
    "# Lambda values for EWC and SI\n",
    "lambda_ewc = 1000\n",
    "lambda_si = 1.0\n",
    "\n",
    "# Function to split data among clients\n",
    "def split_data(X, y_super, y_sub, num_clients):\n",
    "    client_data = []\n",
    "    data_per_client = len(X) // num_clients\n",
    "    for i in range(num_clients):\n",
    "        start_idx = i * data_per_client\n",
    "        end_idx = (i + 1) * data_per_client if i < num_clients - 1 else len(X)\n",
    "        X_client = X[start_idx:end_idx]\n",
    "        y_client_super = y_super[start_idx:end_idx]\n",
    "        y_client_sub = y_sub[start_idx:end_idx]\n",
    "        client_data.append((X_client, y_client_super, y_client_sub))\n",
    "    return client_data\n",
    "\n",
    "# Split the training data among clients\n",
    "client_data = split_data(X_train, y_train_super, y_train_sub, num_clients)\n",
    "\n",
    "# Function to clone a model and set weights\n",
    "def clone_model_weights(model):\n",
    "    cloned_model = tf.keras.models.clone_model(model)\n",
    "    cloned_model.set_weights(model.get_weights())\n",
    "    return cloned_model\n",
    "\n",
    "# Function to average client weights\n",
    "def average_weights(client_weights):\n",
    "    avg_weights = []\n",
    "    for weights in zip(*client_weights):\n",
    "        avg = np.mean(weights, axis=0)\n",
    "        avg_weights.append(avg)\n",
    "    return avg_weights\n",
    "\n",
    "# Define Federated Learning with EWC and SI\n",
    "def federated_training(model_type, create_model_fn, input_shape, num_classes_super, num_classes_sub, classes_super, classes_sub):\n",
    "    print(f\"--- Federated Training for {model_type} ---\")\n",
    "    \n",
    "    # Initialize the global model\n",
    "    global_model_super = create_model_fn(input_shape, num_classes_super)\n",
    "    global_weights_super = global_model_super.get_weights()\n",
    "    \n",
    "    # Initialize the global models for subdiagnostic tasks\n",
    "    global_model_sub = modify_model_for_subdiagnostic(global_model_super, num_classes_sub)\n",
    "    global_weights_sub = global_model_sub.get_weights()\n",
    "    \n",
    "    for round_num in range(communication_rounds):\n",
    "        print(f\"Communication Round {round_num+1}/{communication_rounds}\")\n",
    "        client_weights_super = []\n",
    "        client_weights_sub = []\n",
    "        \n",
    "        for client_idx, (X_client, y_client_super, y_client_sub) in enumerate(client_data):\n",
    "            print(f\" - Client {client_idx+1}/{num_clients} local training\")\n",
    "            \n",
    "            # Clone global models\n",
    "            client_model_super = clone_model_weights(global_model_super)\n",
    "            client_model_sub = clone_model_weights(global_model_sub)\n",
    "            \n",
    "            # --------------------\n",
    "            # Local Training on Superdiagnostic Task\n",
    "            # --------------------\n",
    "            client_model_super.compile(\n",
    "                optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=[macro_f1]\n",
    "            )\n",
    "            client_model_super.fit(\n",
    "                X_client, y_client_super,\n",
    "                epochs=local_epochs,\n",
    "                batch_size=local_batch_size,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Initialize EWC for the client\n",
    "            exclude_params_super = [id(w) for w in client_model_super.layers[-1].trainable_weights]\n",
    "            ewc_client = EWC(client_model_super, X_client, y_client_super, exclude_params=exclude_params_super)\n",
    "            \n",
    "            # --------------------\n",
    "            # Local Training on Subdiagnostic Task with EWC\n",
    "            # --------------------\n",
    "            client_model_sub = modify_model_for_subdiagnostic(client_model_super, num_classes_sub)\n",
    "            \n",
    "            # Define EWC loss for subdiagnostic task\n",
    "            def ewc_loss_sub(y_true, y_pred):\n",
    "                task_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "                ewc_penalty = ewc_client.penalty(client_model_sub)\n",
    "                total_loss = task_loss + (lambda_ewc / 2) * ewc_penalty\n",
    "                return total_loss\n",
    "            \n",
    "            client_model_sub.compile(\n",
    "                optimizer='adam',\n",
    "                loss=ewc_loss_sub,\n",
    "                metrics=[macro_f1]\n",
    "            )\n",
    "            client_model_sub.fit(\n",
    "                X_client, y_client_sub,\n",
    "                epochs=local_epochs,\n",
    "                batch_size=local_batch_size,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Initialize SI for the client\n",
    "            exclude_params_sub = [id(w) for w in client_model_sub.layers[-1].trainable_weights]\n",
    "            si_client = SI(client_model_sub, damping_factor=0.1, exclude_params=exclude_params_sub)\n",
    "            \n",
    "            # --------------------\n",
    "            # Local Training on Subdiagnostic Task with SI\n",
    "            # --------------------\n",
    "            # Define SI loss for subdiagnostic task\n",
    "            def si_loss_sub(y_true, y_pred):\n",
    "                task_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "                si_penalty = si_client.penalty(client_model_sub)\n",
    "                total_loss = task_loss + (lambda_si / 2) * si_penalty\n",
    "                return total_loss\n",
    "            \n",
    "            client_model_sub.compile(\n",
    "                optimizer='adam',\n",
    "                loss=si_loss_sub,\n",
    "                metrics=[macro_f1]\n",
    "            )\n",
    "            client_model_sub.fit(\n",
    "                X_client, y_client_sub,\n",
    "                epochs=local_epochs,\n",
    "                batch_size=local_batch_size,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Update EWC and SI\n",
    "            # For EWC, we've already computed the penalty during training\n",
    "            # For SI, update omega after training\n",
    "            si_client.update_omega(client_model_sub)\n",
    "            \n",
    "            # Collect client weights\n",
    "            client_weights_super.append(client_model_super.get_weights())\n",
    "            client_weights_sub.append(client_model_sub.get_weights())\n",
    "        \n",
    "        # Aggregate client weights to update global models\n",
    "        global_weights_super = average_weights(client_weights_super)\n",
    "        global_weights_sub = average_weights(client_weights_sub)\n",
    "        \n",
    "        # Set the aggregated weights to global models\n",
    "        global_model_super.set_weights(global_weights_super)\n",
    "        global_model_sub.set_weights(global_weights_sub)\n",
    "    \n",
    "    # After federated training, evaluate the global models\n",
    "    print(f\"--- Evaluation for {model_type} after Federated Learning ---\")\n",
    "    \n",
    "    # Evaluate on Superdiagnostic Task\n",
    "    print(f\"Evaluating Global {model_type} Model on Superdiagnostic Task:\")\n",
    "    super_report = evaluate_model(global_model_super, X_test, y_test_super, classes_super)\n",
    "    \n",
    "    # Evaluate on Subdiagnostic Task\n",
    "    print(f\"Evaluating Global {model_type} Model on Subdiagnostic Task:\")\n",
    "    global_model_sub = modify_model_for_subdiagnostic(global_model_super, num_classes_sub)\n",
    "    sub_report = evaluate_model(global_model_sub, X_test, y_test_sub, classes_sub)\n",
    "    \n",
    "    return super_report, sub_report\n",
    "\n",
    "# ----------------------------------------\n",
    "# Start Federated Training for CNN, ResNet, ViT\n",
    "# ----------------------------------------\n",
    "\n",
    "# Initialize a dictionary to store federated reports\n",
    "federated_reports = {\n",
    "    'Model': [],\n",
    "    'Task': [],\n",
    "    'Macro F1-score': []\n",
    "}\n",
    "\n",
    "# List of models to federate\n",
    "models_to_federate = [\n",
    "    ('CNN', create_cnn_model),\n",
    "    ('ResNet', create_resnet_model),\n",
    "    ('ViT', create_vit_model)\n",
    "]\n",
    "\n",
    "for model_name, create_model_fn in models_to_federate:\n",
    "    # Determine number of classes based on model type\n",
    "    if model_name in ['CNN', 'ResNet', 'ViT']:\n",
    "        num_classes_sub = y_train_sub.shape[1]\n",
    "    \n",
    "    # Federated training\n",
    "    super_report, sub_report = federated_training(\n",
    "        model_type=model_name,\n",
    "        create_model_fn=create_model_fn,\n",
    "        input_shape=input_shape,\n",
    "        num_classes_super=num_classes_super,\n",
    "        num_classes_sub=num_classes_sub,\n",
    "        classes_super=classes_super,\n",
    "        classes_sub=classes_sub\n",
    "    )\n",
    "    \n",
    "    # Store the reports\n",
    "    federated_reports['Model'].extend([model_name, model_name])\n",
    "    federated_reports['Task'].extend(['Federated Superdiagnostic', 'Federated Subdiagnostic'])\n",
    "    federated_reports['Macro F1-score'].extend([\n",
    "        get_macro_f1(super_report),\n",
    "        get_macro_f1(sub_report)\n",
    "    ])\n",
    "\n",
    "# ----------------------------------------\n",
    "# Update and Publish the Summary Table with Federated Results\n",
    "# ----------------------------------------\n",
    "\n",
    "# Convert federated reports to DataFrame\n",
    "federated_df = pd.DataFrame(federated_reports)\n",
    "\n",
    "# Append federated results to the existing summary table\n",
    "results = results.append(federated_df, ignore_index=True)\n",
    "\n",
    "print(\"Updated Summary of Classification Performance with Federated Learning:\")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
