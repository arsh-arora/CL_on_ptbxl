{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wfdb\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path to PTBXL, please adjust accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '/Users/aroraji/Desktop/PersonalProjects/ML/GermanyProject/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATASET_PATH, 'ptbxl_database.csv'), index_col='ecg_id')\n",
    "scp_df = pd.read_csv(os.path.join(DATASET_PATH, 'scp_statements.csv'), index_col=0)\n",
    "df['scp_codes'] = df['scp_codes'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_diagnostic(y_dic):\n",
    "    tmp = []\n",
    "    for key in y_dic.keys():\n",
    "        if key in scp_df.index:\n",
    "            diagnostic_class = scp_df.loc[key]['diagnostic_class']\n",
    "            if isinstance(diagnostic_class, str) and diagnostic_class.strip():\n",
    "                tmp.append(diagnostic_class)\n",
    "    return list(set(tmp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diagnostic_superclass'] = df['scp_codes'].apply(aggregate_diagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecg_id\n",
      "1    [NORM]\n",
      "2    [NORM]\n",
      "3    [NORM]\n",
      "4    [NORM]\n",
      "5    [NORM]\n",
      "Name: diagnostic_superclass, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['diagnostic_superclass'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['diagnostic_superclass'].map(len) > 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           [NORM]\n",
      "1           [NORM]\n",
      "2           [NORM]\n",
      "3           [NORM]\n",
      "4           [NORM]\n",
      "          ...     \n",
      "995    [STTC, HYP]\n",
      "996         [NORM]\n",
      "997         [NORM]\n",
      "998         [NORM]\n",
      "999           [CD]\n",
      "Name: diagnostic_superclass, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['diagnostic_superclass'].head(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_subdiagnostic(y_dic):\n",
    "    tmp = []\n",
    "    for key in y_dic.keys():\n",
    "        if key in scp_df.index:\n",
    "            diagnostic_subclass = scp_df.loc[key]['diagnostic_subclass']\n",
    "            if isinstance(diagnostic_subclass, str) and diagnostic_subclass.strip():\n",
    "                tmp.append(diagnostic_subclass)\n",
    "    return list(set(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diagnostic_subclass'] = df['scp_codes'].apply(aggregate_subdiagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           [NORM]\n",
      "1           [NORM]\n",
      "2           [NORM]\n",
      "3           [NORM]\n",
      "4           [NORM]\n",
      "          ...     \n",
      "995    [STTC, LVH]\n",
      "996         [NORM]\n",
      "997         [NORM]\n",
      "998         [NORM]\n",
      "999        [CRBBB]\n",
      "Name: diagnostic_subclass, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['diagnostic_subclass'].head(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining the data and End Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diagnostic_superclass'] = df['diagnostic_superclass'].apply(\n",
    "    lambda lst: [x for x in lst if isinstance(x, str) and x.strip()]\n",
    ")\n",
    "df = df[df['diagnostic_superclass'].map(len) > 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diagnostic_subclass'] = df['diagnostic_subclass'].apply(\n",
    "    lambda lst: [x for x in lst if isinstance(x, str) and x.strip()]\n",
    ")\n",
    "df = df[df['diagnostic_subclass'].map(len) > 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique diagnostic classes:\n",
      "['STTC' 'NORM' 'MI' 'HYP' 'CD' nan]\n"
     ]
    }
   ],
   "source": [
    "scp_df = pd.read_csv(\n",
    "    os.path.join(DATASET_PATH, 'scp_statements.csv'),\n",
    "    index_col=0\n",
    ")\n",
    "print(\"Unique diagnostic classes:\")\n",
    "print(scp_df['diagnostic_class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [NORM]\n",
      "1    [NORM]\n",
      "2    [NORM]\n",
      "3    [NORM]\n",
      "4    [NORM]\n",
      "Name: diagnostic_superclass, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['diagnostic_superclass'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           [NORM]\n",
      "1           [NORM]\n",
      "2           [NORM]\n",
      "3           [NORM]\n",
      "4           [NORM]\n",
      "          ...     \n",
      "995    [STTC, LVH]\n",
      "996         [NORM]\n",
      "997         [NORM]\n",
      "998         [NORM]\n",
      "999        [CRBBB]\n",
      "Name: diagnostic_subclass, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['diagnostic_subclass'].head(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Multilabel Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Classes:\n",
      "['AMI' 'CD' 'CLBBB' 'CRBBB' 'HYP' 'ILBBB' 'IMI' 'IRBBB' 'ISCA' 'ISCI'\n",
      " 'ISC_' 'IVCD' 'LAFB/LPFB' 'LAO/LAE' 'LMI' 'LVH' 'MI' 'NORM' 'NST_' 'PMI'\n",
      " 'RAO/RAE' 'RVH' 'SEHYP' 'STTC' 'WPW' '_AVB']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "all_labels = df['diagnostic_superclass'].tolist() + df['diagnostic_subclass'].tolist()\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(all_labels)\n",
    "labels = mlb.transform(df['diagnostic_superclass'])  # For initial labeling\n",
    "\n",
    "print(\"All Classes:\")\n",
    "print(mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filename'] = df['filename_hr'].apply(lambda x: os.path.join(DATASET_PATH, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continued Preprocessing of Signals (Plus Downsampling to reduce Computations for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_signals(df):\n",
    "    signals = []\n",
    "    for filename in df['filename']:\n",
    "        signal, fields = wfdb.rdsamp(filename)\n",
    "        signals.append(signal)\n",
    "    return np.array(signals)\n",
    "\n",
    "X = load_signals(df)\n",
    "\n",
    "def downsample_signals(X, target_length=1000):\n",
    "    X_downsampled = signal.resample(X, target_length, axis=1)\n",
    "    return X_downsampled\n",
    "\n",
    "X = downsample_signals(X, target_length=1000)\n",
    "\n",
    "def normalize_signals(X):\n",
    "    N, L, C = X.shape\n",
    "    X_normalized = np.zeros_like(X)\n",
    "    for i in range(N):\n",
    "        scaler = StandardScaler()\n",
    "        X_normalized[i] = scaler.fit_transform(X[i])\n",
    "    return X_normalized\n",
    "\n",
    "X = normalize_signals(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "train_indices = df[df.strat_fold < 10].index\n",
    "test_indices = df[df.strat_fold == 10].index\n",
    "\n",
    "X_train = X[train_indices]\n",
    "X_test = X[test_indices]\n",
    "\n",
    "y_train = labels[train_indices]\n",
    "y_test = labels[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb_super = MultiLabelBinarizer()\n",
    "y_super = mlb_super.fit_transform(df['diagnostic_superclass'])\n",
    "mlb_sub = MultiLabelBinarizer()\n",
    "y_sub = mlb_sub.fit_transform(df['diagnostic_subclass'])\n",
    "\n",
    "all_classes = list(set(mlb_super.classes_).union(set(mlb_sub.classes_)))\n",
    "mlb_all = MultiLabelBinarizer(classes=all_classes)\n",
    "mlb_all.fit(all_classes)\n",
    "\n",
    "y_super_all = mlb_all.transform(df['diagnostic_superclass'])\n",
    "y_sub_all = mlb_all.transform(df['diagnostic_subclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X.shape[1:]\n",
    "num_classes = len(mlb_all.classes_)\n",
    "\n",
    "def build_resnet(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(64, kernel_size=7, strides=2, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(x)\n",
    "\n",
    "    def res_block(x, filters, kernel_size=3, stride=1):\n",
    "        shortcut = x\n",
    "        x = layers.Conv1D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.Conv1D(filters, kernel_size, strides=1, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        if stride != 1 or x.shape[-1] != shortcut.shape[-1]:\n",
    "            shortcut = layers.Conv1D(filters, kernel_size=1, strides=stride, padding='same')(shortcut)\n",
    "            shortcut = layers.BatchNormalization()(shortcut)\n",
    "        x = layers.add([x, shortcut])\n",
    "        x = layers.ReLU()(x)\n",
    "        return x\n",
    "\n",
    "    x = res_block(x, 64)\n",
    "    x = res_block(x, 64)\n",
    "    \n",
    "    x = res_block(x, 128, stride=2)\n",
    "    x = res_block(x, 128)\n",
    "    \n",
    "    x = res_block(x, 256, stride=2)\n",
    "    x = res_block(x, 256)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def build_model():\n",
    "    return build_resnet(input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# epochs = 1\n",
    "\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     batch_size=batch_size,\n",
    "#     epochs=epochs,\n",
    "#     validation_data=(X_test, y_test)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 4))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(history.history['accuracy'], 'b-', label='Training Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], 'r-', label='Validation Accuracy')\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(history.history['loss'], 'b-', label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], 'r-', label='Validation Loss')\n",
    "# plt.title('Model Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ecg/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def build_cnn(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=7, activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "input_shape = X_train.shape[1:]\n",
    "num_classes = y_train.shape[1]\n",
    "model = build_cnn(input_shape, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# batch_size = 32\n",
    "# epochs = 10\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     batch_size=batch_size,\n",
    "#     epochs=epochs,\n",
    "#     validation_data=(X_test, y_test)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    position_embedding = layers.Embedding(\n",
    "        input_dim=input_shape[0],\n",
    "        output_dim=input_shape[1]\n",
    "    )\n",
    "    positions = tf.range(start=0, limit=input_shape[0], delta=1)\n",
    "    positions = position_embedding(positions)\n",
    "    x = x + positions\n",
    "\n",
    "    num_heads = 4\n",
    "    ff_dim = 128\n",
    "    def transformer_encoder_block(inputs, head_size, ff_dim, dropout=0.1):\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "        x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=head_size, dropout=dropout)(x, x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        x = layers.Add()([x, inputs])\n",
    "\n",
    "        x_ff = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x_ff = layers.Dense(ff_dim, activation='relu')(x_ff)\n",
    "        x_ff = layers.Dropout(dropout)(x_ff)\n",
    "        x_ff = layers.Dense(inputs.shape[-1], activation='relu')(x_ff)\n",
    "        x_ff = layers.Dropout(dropout)(x_ff)\n",
    "        x = layers.Add()([x, x_ff])\n",
    "\n",
    "        return x\n",
    "    x = transformer_encoder_block(x, head_size=input_shape[1], ff_dim=ff_dim)\n",
    "    x = transformer_encoder_block(x, head_size=input_shape[1], ff_dim=ff_dim)\n",
    "    x = transformer_encoder_block(x, head_size=input_shape[1], ff_dim=ff_dim)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = X_train.shape[1:] \n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "model = build_transformer(input_shape, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# batch_size = 128\n",
    "# epochs = 5\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     batch_size=batch_size,\n",
    "#     epochs=epochs,\n",
    "#     validation_data=(X_test, y_test)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of CL benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 - Superdiagnostic Classification\n",
      "Classes: ['CD' 'HYP' 'MI' 'NORM' 'STTC']\n",
      "\n",
      "Task 2 - Subdiagnostic Classification\n",
      "Classes: ['AMI' 'CLBBB' 'CRBBB' 'ILBBB' 'IMI' 'IRBBB' 'ISCA' 'ISCI' 'ISC_' 'IVCD'\n",
      " 'LAFB/LPFB' 'LAO/LAE' 'LMI' 'LVH' 'NORM' 'NST_' 'PMI' 'RAO/RAE' 'RVH'\n",
      " 'SEHYP' 'STTC' 'WPW' '_AVB']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tasks = [\n",
    "    {\n",
    "        'X': X,\n",
    "        'y': y_super_all,\n",
    "        'classes': mlb_super.classes_,\n",
    "        'task_name': 'Superdiagnostic Classification'\n",
    "    },\n",
    "    {\n",
    "        'X': X,\n",
    "        'y': y_sub_all,\n",
    "        'classes': mlb_sub.classes_,\n",
    "        'task_name': 'Subdiagnostic Classification'\n",
    "    }\n",
    "]\n",
    "\n",
    "for idx, task in enumerate(tasks):\n",
    "    print(f\"Task {idx+1} - {task['task_name']}\")\n",
    "    print(\"Classes:\", task['classes'])\n",
    "    print()\n",
    "\n",
    "for task in tasks:\n",
    "    X_train_task, X_test_task, y_train_task, y_test_task = train_test_split(\n",
    "        task['X'], task['y'], test_size=0.2, random_state=42\n",
    "    )\n",
    "    task['X_train'] = X_train_task\n",
    "    task['X_test'] = X_test_task\n",
    "    task['y_train'] = y_train_task\n",
    "    task['y_test'] = y_test_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Elastic Weights Consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Fisher Information Matrix Computation**\n",
    "$$\n",
    "F = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\nabla_\\theta L(y_i, f(x_i; \\theta)) \\right)^2\n",
    "$$\n",
    "\n",
    "2. **EWC Loss Function**\n",
    "$$\n",
    "L_{\\text{EWC}}(\\theta) = L_{\\text{current}}(\\theta) + \\sum_{k} \\frac{\\lambda}{2} \\sum_{j} F_{k}^{(j)} \\left( \\theta^{(j)} - \\theta_k^{*(j)} \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EWC:\n",
    "    def __init__(self, model, lambda_ewc=1000):\n",
    "        self.model = model\n",
    "        self.lambda_ewc = lambda_ewc\n",
    "        self.fisher_matrices = []\n",
    "        self.star_vars = []\n",
    "\n",
    "    def compute_fisher(self, X, y, batch_size=32):\n",
    "        fisher = [np.zeros(var.shape.as_list(), dtype=np.float32) for var in self.model.trainable_variables]\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X, y)).batch(batch_size)\n",
    "        for X_batch, y_batch in dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                preds = self.model(X_batch, training=False)\n",
    "                labels = tf.cast(y_batch, dtype=tf.float32)  # Cast labels to float32\n",
    "                loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=preds))\n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            for n, grad in enumerate(grads):\n",
    "                fisher[n] += np.square(grad.numpy())\n",
    "        num_batches = len(list(dataset))\n",
    "        for n in range(len(fisher)):\n",
    "            fisher[n] /= num_batches\n",
    "        return fisher\n",
    "\n",
    "    def consolidate(self, X, y):\n",
    "        fisher = self.compute_fisher(X, y)\n",
    "        self.fisher_matrices.append(fisher)\n",
    "        self.star_vars.append([var.numpy() for var in self.model.trainable_variables])\n",
    "\n",
    "    def ewc_loss(self):\n",
    "        if not self.fisher_matrices:\n",
    "            return 0\n",
    "        else:\n",
    "            ewc_loss = 0\n",
    "            for task in range(len(self.fisher_matrices)):\n",
    "                for var, var_star, fisher in zip(self.model.trainable_variables, self.star_vars[task], self.fisher_matrices[task]):\n",
    "                    ewc_loss += (self.lambda_ewc / 2) * tf.reduce_sum(\n",
    "                        tf.multiply(tf.convert_to_tensor(fisher), tf.square(var - var_star))\n",
    "                    )\n",
    "            return ewc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Synaptic Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Parameter change:\n",
    "\n",
    "   $$\n",
    "   \\Delta \\theta_i = \\theta_i - \\theta_i^{\\text{prev}}\n",
    "   $$\n",
    "\n",
    "2. Accumulating importance:\n",
    "\n",
    "   $$\n",
    "   w_i \\leftarrow w_i + g_i \\cdot \\Delta \\theta_i\n",
    "   $$\n",
    "\n",
    "3. Update Omega:\n",
    "\n",
    "   $$\n",
    "   \\Omega_i \\leftarrow \\Omega_i + \\frac{w_i}{(\\Delta \\theta_i)^2 + \\epsilon}\n",
    "   $$\n",
    "\n",
    "4. SI loss function:\n",
    "\n",
    "   $$\n",
    "   L_{\\text{SI}} = L_{\\text{current}} + \\lambda \\sum_i \\Omega_i (\\theta_i - \\theta_i^{\\text{prev}})^2\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SI:\n",
    "    def __init__(self, model, lambda_si=1.0, epsilon=0.1):\n",
    "        self.model = model\n",
    "        self.lambda_si = lambda_si\n",
    "        self.epsilon = epsilon\n",
    "        self.prev_params = [var.numpy() for var in self.model.trainable_variables]\n",
    "        self.w = [np.zeros(var.shape, dtype=np.float32) for var in self.model.trainable_variables]\n",
    "        self.omega = [np.zeros(var.shape, dtype=np.float32) for var in self.model.trainable_variables]\n",
    "\n",
    "    def update_omega(self):\n",
    "        delta_params = [var.numpy() - prev for var, prev in zip(self.model.trainable_variables, self.prev_params)]\n",
    "        for i in range(len(self.omega)):\n",
    "            self.omega[i] += self.w[i] / (np.square(delta_params[i]) + self.epsilon)\n",
    "        self.w = [np.zeros_like(var) for var in self.model.trainable_variables]\n",
    "        self.prev_params = [var.numpy() for var in self.model.trainable_variables]\n",
    "\n",
    "    def accumulate_importance(self, grads):\n",
    "        for i in range(len(grads)):\n",
    "            self.w[i] += grads[i].numpy() * (self.model.trainable_variables[i].numpy() - self.prev_params[i])\n",
    "\n",
    "    def si_loss(self):\n",
    "        loss = 0\n",
    "        for var, var_prev, omega in zip(self.model.trainable_variables, self.prev_params, self.omega):\n",
    "            loss += tf.reduce_sum(omega.astype(np.float32) * tf.square(var - var_prev))\n",
    "        return self.lambda_si * loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Setups\n",
    "\n",
    "### Please pardon the bad results they are due to training over a single epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EWC Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on Task 1 with EWC: Superdiagnostic Classification\n",
      "Test Loss: 0.6661294102668762, Test Accuracy: 0.47106125950813293\n",
      "\n",
      "Training on Task 2 with EWC: Subdiagnostic Classification\n",
      "Test Loss: 0.6889272332191467, Test Accuracy: 0.24824687838554382\n"
     ]
    }
   ],
   "source": [
    "model_ewc = build_model()\n",
    "model_si = build_model()\n",
    "\n",
    "optimizer_ewc = tf.keras.optimizers.Adam()\n",
    "optimizer_si = tf.keras.optimizers.Adam()\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "ewc = EWC(model_ewc, lambda_ewc=1000)\n",
    "si = SI(model_si, lambda_si=1.0, epsilon=0.1)\n",
    "\n",
    "for task_idx, task in enumerate(tasks):\n",
    "    print(f\"\\nTraining on Task {task_idx+1} with EWC: {task['task_name']}\")\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((task['X_train'], task['y_train'])).batch(batch_size)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((task['X_test'], task['y_test'])).batch(batch_size)\n",
    "\n",
    "    relevant_class_indices = [mlb_all.classes_.tolist().index(c) for c in task['classes']]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for X_batch, y_batch in train_dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model_ewc(X_batch, training=True)\n",
    "                y_batch_task = tf.gather(y_batch, relevant_class_indices, axis=1)\n",
    "                logits_task = tf.gather(logits, relevant_class_indices, axis=1)\n",
    "                y_batch_task = tf.cast(y_batch_task, dtype=tf.float32)\n",
    "                loss = loss_fn(y_batch_task, logits_task)\n",
    "                ewc_penalty = ewc.ewc_loss()\n",
    "                total_loss = loss + ewc_penalty\n",
    "            grads = tape.gradient(total_loss, model_ewc.trainable_variables)\n",
    "            optimizer_ewc.apply_gradients(zip(grads, model_ewc.trainable_variables))\n",
    "    ewc.consolidate(task['X_train'], task['y_train'])\n",
    "\n",
    "    test_loss = tf.keras.metrics.Mean()\n",
    "    test_accuracy = tf.keras.metrics.BinaryAccuracy()\n",
    "    for X_batch, y_batch in test_dataset:\n",
    "        logits = model_ewc(X_batch, training=False)\n",
    "        y_batch_task = tf.gather(y_batch, relevant_class_indices, axis=1)\n",
    "        logits_task = tf.gather(logits, relevant_class_indices, axis=1)\n",
    "        y_batch_task = tf.cast(y_batch_task, dtype=tf.float32)\n",
    "        loss = loss_fn(y_batch_task, logits_task)\n",
    "        test_loss(loss)\n",
    "        test_accuracy.update_state(y_batch_task, tf.sigmoid(logits_task))\n",
    "    print(f\"Test Loss: {test_loss.result()}, Test Accuracy: {test_accuracy.result()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SI Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on Task 1 with SI: Superdiagnostic Classification\n",
      "Test Loss: 0.6764861345291138, Test Accuracy: 0.6427768468856812\n",
      "\n",
      "Training on Task 2 with SI: Subdiagnostic Classification\n",
      "Test Loss: 0.6928368806838989, Test Accuracy: 0.9415207505226135\n"
     ]
    }
   ],
   "source": [
    "for task_idx, task in enumerate(tasks):\n",
    "    print(f\"\\nTraining on Task {task_idx+1} with SI: {task['task_name']}\")\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((task['X_train'], task['y_train'])).batch(batch_size)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((task['X_test'], task['y_test'])).batch(batch_size)\n",
    "\n",
    "    relevant_class_indices = [mlb_all.classes_.tolist().index(c) for c in task['classes']]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for X_batch, y_batch in train_dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model_si(X_batch, training=True)\n",
    "                y_batch_task = tf.gather(y_batch, relevant_class_indices, axis=1)\n",
    "                logits_task = tf.gather(logits, relevant_class_indices, axis=1)\n",
    "                y_batch_task = tf.cast(y_batch_task, dtype=tf.float32)\n",
    "                loss = loss_fn(y_batch_task, logits_task)\n",
    "                si_penalty = si.si_loss()\n",
    "                total_loss = loss + si_penalty\n",
    "            grads = tape.gradient(total_loss, model_si.trainable_variables)\n",
    "            si.accumulate_importance(grads)\n",
    "            optimizer_si.apply_gradients(zip(grads, model_si.trainable_variables))\n",
    "    si.update_omega()\n",
    "\n",
    "    test_loss = tf.keras.metrics.Mean()\n",
    "    test_accuracy = tf.keras.metrics.BinaryAccuracy()\n",
    "    for X_batch, y_batch in test_dataset:\n",
    "        logits = model_si(X_batch, training=False)\n",
    "        y_batch_task = tf.gather(y_batch, relevant_class_indices, axis=1)\n",
    "        logits_task = tf.gather(logits, relevant_class_indices, axis=1)\n",
    "        y_batch_task = tf.cast(y_batch_task, dtype=tf.float32)\n",
    "        loss = loss_fn(y_batch_task, logits_task)\n",
    "        test_loss(loss)\n",
    "        test_accuracy.update_state(y_batch_task, tf.sigmoid(logits_task))\n",
    "    print(f\"Test Loss: {test_loss.result()}, Test Accuracy: {test_accuracy.result()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
